package cuda

/*
 THIS FILE IS AUTO-GENERATED BY CUDA2GO.
 EDITING IS FUTILE.
*/

import (
	"github.com/mumax/3/cuda/cu"
	"github.com/mumax/3/timer"
	"sync"
	"unsafe"
)

// CUDA handle for addexchangefourthorder kernel
var addexchangefourthorder_code cu.Function

// Stores the arguments for addexchangefourthorder kernel invocation
type addexchangefourthorder_args_t struct {
	arg_Bx                unsafe.Pointer
	arg_By                unsafe.Pointer
	arg_Bz                unsafe.Pointer
	arg_mx                unsafe.Pointer
	arg_my                unsafe.Pointer
	arg_mz                unsafe.Pointer
	arg_Ms_               unsafe.Pointer
	arg_Ms_mul            float32
	arg_aSecondOrderLUT2d unsafe.Pointer
	arg_aFourthOrderLUT2d unsafe.Pointer
	arg_regions           unsafe.Pointer
	arg_cx                float32
	arg_cy                float32
	arg_cz                float32
	arg_Nx                int
	arg_Ny                int
	arg_Nz                int
	arg_PBC               byte
	argptr                [18]unsafe.Pointer
	sync.Mutex
}

// Stores the arguments for addexchangefourthorder kernel invocation
var addexchangefourthorder_args addexchangefourthorder_args_t

func init() {
	// CUDA driver kernel call wants pointers to arguments, set them up once.
	addexchangefourthorder_args.argptr[0] = unsafe.Pointer(&addexchangefourthorder_args.arg_Bx)
	addexchangefourthorder_args.argptr[1] = unsafe.Pointer(&addexchangefourthorder_args.arg_By)
	addexchangefourthorder_args.argptr[2] = unsafe.Pointer(&addexchangefourthorder_args.arg_Bz)
	addexchangefourthorder_args.argptr[3] = unsafe.Pointer(&addexchangefourthorder_args.arg_mx)
	addexchangefourthorder_args.argptr[4] = unsafe.Pointer(&addexchangefourthorder_args.arg_my)
	addexchangefourthorder_args.argptr[5] = unsafe.Pointer(&addexchangefourthorder_args.arg_mz)
	addexchangefourthorder_args.argptr[6] = unsafe.Pointer(&addexchangefourthorder_args.arg_Ms_)
	addexchangefourthorder_args.argptr[7] = unsafe.Pointer(&addexchangefourthorder_args.arg_Ms_mul)
	addexchangefourthorder_args.argptr[8] = unsafe.Pointer(&addexchangefourthorder_args.arg_aSecondOrderLUT2d)
	addexchangefourthorder_args.argptr[9] = unsafe.Pointer(&addexchangefourthorder_args.arg_aFourthOrderLUT2d)
	addexchangefourthorder_args.argptr[10] = unsafe.Pointer(&addexchangefourthorder_args.arg_regions)
	addexchangefourthorder_args.argptr[11] = unsafe.Pointer(&addexchangefourthorder_args.arg_cx)
	addexchangefourthorder_args.argptr[12] = unsafe.Pointer(&addexchangefourthorder_args.arg_cy)
	addexchangefourthorder_args.argptr[13] = unsafe.Pointer(&addexchangefourthorder_args.arg_cz)
	addexchangefourthorder_args.argptr[14] = unsafe.Pointer(&addexchangefourthorder_args.arg_Nx)
	addexchangefourthorder_args.argptr[15] = unsafe.Pointer(&addexchangefourthorder_args.arg_Ny)
	addexchangefourthorder_args.argptr[16] = unsafe.Pointer(&addexchangefourthorder_args.arg_Nz)
	addexchangefourthorder_args.argptr[17] = unsafe.Pointer(&addexchangefourthorder_args.arg_PBC)
}

// Wrapper for addexchangefourthorder CUDA kernel, asynchronous.
func k_addexchangefourthorder_async(Bx unsafe.Pointer, By unsafe.Pointer, Bz unsafe.Pointer, mx unsafe.Pointer, my unsafe.Pointer, mz unsafe.Pointer, Ms_ unsafe.Pointer, Ms_mul float32, aSecondOrderLUT2d unsafe.Pointer, aFourthOrderLUT2d unsafe.Pointer, regions unsafe.Pointer, cx float32, cy float32, cz float32, Nx int, Ny int, Nz int, PBC byte, cfg *config) {
	if Synchronous { // debug
		Sync()
		timer.Start("addexchangefourthorder")
	}

	addexchangefourthorder_args.Lock()
	defer addexchangefourthorder_args.Unlock()

	if addexchangefourthorder_code == 0 {
		addexchangefourthorder_code = fatbinLoad(addexchangefourthorder_map, "addexchangefourthorder")
	}

	addexchangefourthorder_args.arg_Bx = Bx
	addexchangefourthorder_args.arg_By = By
	addexchangefourthorder_args.arg_Bz = Bz
	addexchangefourthorder_args.arg_mx = mx
	addexchangefourthorder_args.arg_my = my
	addexchangefourthorder_args.arg_mz = mz
	addexchangefourthorder_args.arg_Ms_ = Ms_
	addexchangefourthorder_args.arg_Ms_mul = Ms_mul
	addexchangefourthorder_args.arg_aSecondOrderLUT2d = aSecondOrderLUT2d
	addexchangefourthorder_args.arg_aFourthOrderLUT2d = aFourthOrderLUT2d
	addexchangefourthorder_args.arg_regions = regions
	addexchangefourthorder_args.arg_cx = cx
	addexchangefourthorder_args.arg_cy = cy
	addexchangefourthorder_args.arg_cz = cz
	addexchangefourthorder_args.arg_Nx = Nx
	addexchangefourthorder_args.arg_Ny = Ny
	addexchangefourthorder_args.arg_Nz = Nz
	addexchangefourthorder_args.arg_PBC = PBC

	args := addexchangefourthorder_args.argptr[:]
	cu.LaunchKernel(addexchangefourthorder_code, cfg.Grid.X, cfg.Grid.Y, cfg.Grid.Z, cfg.Block.X, cfg.Block.Y, cfg.Block.Z, 0, stream0, args)

	if Synchronous { // debug
		Sync()
		timer.Stop("addexchangefourthorder")
	}
}

// maps compute capability on PTX code for addexchangefourthorder kernel.
var addexchangefourthorder_map = map[int]string{0: "",
	35: addexchangefourthorder_ptx_35,
	37: addexchangefourthorder_ptx_37,
	50: addexchangefourthorder_ptx_50,
	52: addexchangefourthorder_ptx_52,
	53: addexchangefourthorder_ptx_53,
	60: addexchangefourthorder_ptx_60,
	61: addexchangefourthorder_ptx_61,
	62: addexchangefourthorder_ptx_62,
	70: addexchangefourthorder_ptx_70,
	80: addexchangefourthorder_ptx_80}

// addexchangefourthorder PTX code for various compute capabilities.
const (
	addexchangefourthorder_ptx_35 = `
.version 7.4
.target sm_35
.address_size 64

	// .globl	addexchangefourthorder

.visible .entry addexchangefourthorder(
	.param .u64 addexchangefourthorder_param_0,
	.param .u64 addexchangefourthorder_param_1,
	.param .u64 addexchangefourthorder_param_2,
	.param .u64 addexchangefourthorder_param_3,
	.param .u64 addexchangefourthorder_param_4,
	.param .u64 addexchangefourthorder_param_5,
	.param .u64 addexchangefourthorder_param_6,
	.param .f32 addexchangefourthorder_param_7,
	.param .u64 addexchangefourthorder_param_8,
	.param .u64 addexchangefourthorder_param_9,
	.param .u64 addexchangefourthorder_param_10,
	.param .f32 addexchangefourthorder_param_11,
	.param .f32 addexchangefourthorder_param_12,
	.param .f32 addexchangefourthorder_param_13,
	.param .u32 addexchangefourthorder_param_14,
	.param .u32 addexchangefourthorder_param_15,
	.param .u32 addexchangefourthorder_param_16,
	.param .u8 addexchangefourthorder_param_17
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<20>;
	.reg .f32 	%f<107>;
	.reg .b32 	%r<71>;
	.reg .b64 	%rd<61>;


	ld.param.u8 	%rs3, [addexchangefourthorder_param_17];
	ld.param.u64 	%rd7, [addexchangefourthorder_param_0];
	ld.param.u64 	%rd8, [addexchangefourthorder_param_1];
	ld.param.u64 	%rd9, [addexchangefourthorder_param_2];
	ld.param.u64 	%rd11, [addexchangefourthorder_param_3];
	ld.param.u64 	%rd12, [addexchangefourthorder_param_4];
	ld.param.u64 	%rd13, [addexchangefourthorder_param_5];
	ld.param.u64 	%rd10, [addexchangefourthorder_param_6];
	ld.param.f32 	%f105, [addexchangefourthorder_param_7];
	ld.param.u64 	%rd14, [addexchangefourthorder_param_8];
	ld.param.u64 	%rd15, [addexchangefourthorder_param_9];
	ld.param.u64 	%rd16, [addexchangefourthorder_param_10];
	ld.param.f32 	%f24, [addexchangefourthorder_param_11];
	ld.param.f32 	%f23, [addexchangefourthorder_param_12];
	ld.param.u32 	%r19, [addexchangefourthorder_param_14];
	ld.param.u32 	%r20, [addexchangefourthorder_param_15];
	ld.param.u32 	%r21, [addexchangefourthorder_param_16];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd15;
	cvta.to.global.u64 	%rd3, %rd16;
	cvta.to.global.u64 	%rd4, %rd13;
	cvta.to.global.u64 	%rd5, %rd12;
	cvta.to.global.u64 	%rd6, %rd11;
	mov.u32 	%r22, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r1, %r23, %r22, %r24;
	mov.u32 	%r25, %ntid.y;
	mov.u32 	%r26, %ctaid.y;
	mov.u32 	%r27, %tid.y;
	mad.lo.s32 	%r2, %r26, %r25, %r27;
	mov.u32 	%r28, %ntid.z;
	mov.u32 	%r29, %ctaid.z;
	mov.u32 	%r30, %tid.z;
	mad.lo.s32 	%r3, %r29, %r28, %r30;
	mul.f32 	%f25, %f24, %f24;
	mov.f32 	%f26, 0f40000000;
	div.rn.f32 	%f1, %f26, %f25;
	mul.f32 	%f27, %f25, %f24;
	mul.f32 	%f28, %f27, %f24;
	div.rn.f32 	%f2, %f26, %f28;
	setp.ge.s32 	%p1, %r1, %r19;
	setp.ge.s32 	%p2, %r2, %r20;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32 	%p4, %r3, %r21;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB0_16;

	mul.lo.s32 	%r4, %r3, %r20;
	add.s32 	%r31, %r4, %r2;
	mul.lo.s32 	%r5, %r31, %r19;
	add.s32 	%r6, %r5, %r1;
	mul.wide.s32 	%rd17, %r6, 4;
	add.s64 	%rd18, %rd6, %rd17;
	add.s64 	%rd19, %rd5, %rd17;
	add.s64 	%rd20, %rd4, %rd17;
	ld.global.nc.f32 	%f3, [%rd18];
	ld.global.nc.f32 	%f4, [%rd19];
	ld.global.nc.f32 	%f5, [%rd20];
	mul.f32 	%f29, %f4, %f4;
	fma.rn.f32 	%f30, %f3, %f3, %f29;
	fma.rn.f32 	%f31, %f5, %f5, %f30;
	setp.eq.f32 	%p6, %f31, 0f00000000;
	@%p6 bra 	$L__BB0_16;

	cvt.s64.s32 	%rd21, %r6;
	add.s64 	%rd22, %rd3, %rd21;
	ld.global.nc.u8 	%rs1, [%rd22];
	and.b16  	%rs2, %rs3, 1;
	setp.eq.s16 	%p7, %rs2, 0;
	add.s32 	%r7, %r1, -2;
	@%p7 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_3;

$L__BB0_4:
	max.s32 	%r68, %r7, 0;
	bra.uni 	$L__BB0_5;

$L__BB0_3:
	rem.s32 	%r32, %r7, %r19;
	add.s32 	%r33, %r32, %r19;
	rem.s32 	%r68, %r33, %r19;

$L__BB0_5:
	cvt.u32.u16 	%r34, %rs1;
	and.b32  	%r35, %r34, 255;
	add.s32 	%r36, %r68, %r5;
	cvt.s64.s32 	%rd23, %r36;
	mul.wide.s32 	%rd24, %r36, 4;
	add.s64 	%rd25, %rd6, %rd24;
	add.s64 	%rd26, %rd5, %rd24;
	add.s64 	%rd27, %rd4, %rd24;
	ld.global.nc.f32 	%f32, [%rd27];
	ld.global.nc.f32 	%f33, [%rd25];
	ld.global.nc.f32 	%f34, [%rd26];
	mul.f32 	%f35, %f34, %f34;
	fma.rn.f32 	%f36, %f33, %f33, %f35;
	fma.rn.f32 	%f37, %f32, %f32, %f36;
	setp.eq.f32 	%p8, %f37, 0f00000000;
	selp.f32 	%f38, %f5, %f32, %p8;
	selp.f32 	%f39, %f4, %f34, %p8;
	selp.f32 	%f40, %f3, %f33, %p8;
	add.s64 	%rd28, %rd3, %rd23;
	ld.global.nc.u8 	%rs5, [%rd28];
	min.u16 	%rs7, %rs5, %rs1;
	cvt.u32.u16 	%r37, %rs7;
	max.u16 	%rs8, %rs5, %rs1;
	cvt.u32.u16 	%r38, %rs8;
	add.s32 	%r39, %r38, 1;
	mul.lo.s32 	%r40, %r39, %r38;
	shr.u32 	%r41, %r40, 1;
	add.s32 	%r42, %r41, %r37;
	mul.wide.s32 	%rd29, %r42, 4;
	add.s64 	%rd30, %rd2, %rd29;
	ld.global.nc.f32 	%f41, [%rd30];
	mul.f32 	%f42, %f2, %f41;
	fma.rn.f32 	%f43, %f42, %f40, 0f00000000;
	fma.rn.f32 	%f44, %f42, %f39, 0f00000000;
	fma.rn.f32 	%f45, %f42, %f38, 0f00000000;
	add.s32 	%r43, %r35, 1;
	mul.lo.s32 	%r44, %r43, %r35;
	shr.u32 	%r45, %r44, 1;
	add.s32 	%r46, %r45, %r35;
	mul.wide.s32 	%rd31, %r46, 4;
	add.s64 	%rd32, %rd2, %rd31;
	mul.f32 	%f46, %f2, 0f40C00000;
	ld.global.nc.f32 	%f47, [%rd32];
	mul.f32 	%f48, %f46, %f47;
	fma.rn.f32 	%f49, %f48, %f3, %f43;
	fma.rn.f32 	%f50, %f48, %f4, %f44;
	fma.rn.f32 	%f51, %f48, %f5, %f45;
	add.s64 	%rd33, %rd1, %rd31;
	add.f32 	%f52, %f1, %f1;
	ld.global.nc.f32 	%f53, [%rd33];
	mul.f32 	%f54, %f52, %f53;
	mul.f32 	%f55, %f3, %f54;
	mul.f32 	%f56, %f4, %f54;
	mul.f32 	%f57, %f5, %f54;
	sub.f32 	%f9, %f49, %f55;
	sub.f32 	%f10, %f50, %f56;
	sub.f32 	%f11, %f51, %f57;
	add.s32 	%r11, %r1, 1;
	@%p7 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_6;

$L__BB0_7:
	add.s32 	%r49, %r19, -1;
	min.s32 	%r69, %r11, %r49;
	bra.uni 	$L__BB0_8;

$L__BB0_6:
	rem.s32 	%r47, %r11, %r19;
	add.s32 	%r48, %r47, %r19;
	rem.s32 	%r69, %r48, %r19;

$L__BB0_8:
	add.s32 	%r50, %r69, %r5;
	cvt.s64.s32 	%rd34, %r50;
	mul.wide.s32 	%rd35, %r50, 4;
	add.s64 	%rd36, %rd6, %rd35;
	add.s64 	%rd37, %rd5, %rd35;
	add.s64 	%rd38, %rd4, %rd35;
	ld.global.nc.f32 	%f58, [%rd38];
	ld.global.nc.f32 	%f59, [%rd36];
	ld.global.nc.f32 	%f60, [%rd37];
	mul.f32 	%f61, %f60, %f60;
	fma.rn.f32 	%f62, %f59, %f59, %f61;
	fma.rn.f32 	%f63, %f58, %f58, %f62;
	setp.eq.f32 	%p10, %f63, 0f00000000;
	selp.f32 	%f64, %f5, %f58, %p10;
	selp.f32 	%f65, %f4, %f60, %p10;
	selp.f32 	%f66, %f3, %f59, %p10;
	add.s64 	%rd39, %rd3, %rd34;
	ld.global.nc.u8 	%rs9, [%rd39];
	min.u16 	%rs12, %rs9, %rs1;
	cvt.u32.u16 	%r51, %rs12;
	max.u16 	%rs13, %rs9, %rs1;
	cvt.u32.u16 	%r52, %rs13;
	add.s32 	%r53, %r52, 1;
	mul.lo.s32 	%r54, %r53, %r52;
	shr.u32 	%r55, %r54, 1;
	add.s32 	%r56, %r55, %r51;
	mul.wide.s32 	%rd40, %r56, 4;
	add.s64 	%rd41, %rd2, %rd40;
	mul.f32 	%f67, %f2, 0f40800000;
	ld.global.nc.f32 	%f68, [%rd41];
	mul.f32 	%f69, %f67, %f68;
	mul.f32 	%f70, %f69, %f66;
	mul.f32 	%f71, %f69, %f65;
	mul.f32 	%f72, %f69, %f64;
	sub.f32 	%f73, %f9, %f70;
	sub.f32 	%f74, %f10, %f71;
	sub.f32 	%f75, %f11, %f72;
	add.s64 	%rd42, %rd1, %rd40;
	ld.global.nc.f32 	%f76, [%rd42];
	mul.f32 	%f77, %f1, %f76;
	fma.rn.f32 	%f12, %f66, %f77, %f73;
	fma.rn.f32 	%f13, %f65, %f77, %f74;
	fma.rn.f32 	%f14, %f64, %f77, %f75;
	and.b16  	%rs14, %rs3, 2;
	setp.eq.s16 	%p11, %rs14, 0;
	add.s32 	%r15, %r2, 2;
	@%p11 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_9;

$L__BB0_10:
	add.s32 	%r59, %r20, -1;
	min.s32 	%r70, %r15, %r59;
	bra.uni 	$L__BB0_11;

$L__BB0_9:
	rem.s32 	%r57, %r15, %r20;
	add.s32 	%r58, %r57, %r20;
	rem.s32 	%r70, %r58, %r20;

$L__BB0_11:
	add.s32 	%r60, %r70, %r4;
	mad.lo.s32 	%r61, %r60, %r19, %r1;
	cvt.s64.s32 	%rd43, %r61;
	mul.wide.s32 	%rd44, %r61, 4;
	add.s64 	%rd45, %rd6, %rd44;
	add.s64 	%rd46, %rd5, %rd44;
	add.s64 	%rd47, %rd4, %rd44;
	ld.global.nc.f32 	%f78, [%rd47];
	ld.global.nc.f32 	%f79, [%rd45];
	ld.global.nc.f32 	%f80, [%rd46];
	mul.f32 	%f81, %f80, %f80;
	fma.rn.f32 	%f82, %f79, %f79, %f81;
	fma.rn.f32 	%f83, %f78, %f78, %f82;
	setp.eq.f32 	%p12, %f83, 0f00000000;
	selp.f32 	%f84, %f5, %f78, %p12;
	selp.f32 	%f85, %f4, %f80, %p12;
	selp.f32 	%f86, %f3, %f79, %p12;
	add.s64 	%rd48, %rd3, %rd43;
	ld.global.nc.u8 	%rs15, [%rd48];
	min.u16 	%rs18, %rs15, %rs1;
	cvt.u32.u16 	%r62, %rs18;
	max.u16 	%rs19, %rs15, %rs1;
	cvt.u32.u16 	%r63, %rs19;
	add.s32 	%r64, %r63, 1;
	mul.lo.s32 	%r65, %r64, %r63;
	shr.u32 	%r66, %r65, 1;
	add.s32 	%r67, %r66, %r62;
	mul.wide.s32 	%rd49, %r67, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.nc.f32 	%f87, [%rd50];
	mul.f32 	%f88, %f23, %f23;
	mul.f32 	%f89, %f88, %f23;
	mul.f32 	%f90, %f89, %f23;
	div.rn.f32 	%f92, %f26, %f90;
	mul.f32 	%f93, %f92, %f87;
	fma.rn.f32 	%f15, %f93, %f86, %f12;
	fma.rn.f32 	%f16, %f93, %f85, %f13;
	fma.rn.f32 	%f17, %f93, %f84, %f14;
	setp.eq.s64 	%p13, %rd10, 0;
	@%p13 bra 	$L__BB0_13;

	cvta.to.global.u64 	%rd51, %rd10;
	add.s64 	%rd53, %rd51, %rd17;
	ld.global.nc.f32 	%f94, [%rd53];
	mul.f32 	%f105, %f94, %f105;

$L__BB0_13:
	setp.eq.f32 	%p14, %f105, 0f00000000;
	mov.f32 	%f106, 0f00000000;
	@%p14 bra 	$L__BB0_15;

	rcp.rn.f32 	%f106, %f105;

$L__BB0_15:
	cvta.to.global.u64 	%rd54, %rd7;
	add.s64 	%rd56, %rd54, %rd17;
	ld.global.f32 	%f96, [%rd56];
	mul.f32 	%f97, %f15, %f106;
	sub.f32 	%f98, %f96, %f97;
	st.global.f32 	[%rd56], %f98;
	cvta.to.global.u64 	%rd57, %rd8;
	add.s64 	%rd58, %rd57, %rd17;
	ld.global.f32 	%f99, [%rd58];
	mul.f32 	%f100, %f16, %f106;
	sub.f32 	%f101, %f99, %f100;
	st.global.f32 	[%rd58], %f101;
	cvta.to.global.u64 	%rd59, %rd9;
	add.s64 	%rd60, %rd59, %rd17;
	ld.global.f32 	%f102, [%rd60];
	mul.f32 	%f103, %f17, %f106;
	sub.f32 	%f104, %f102, %f103;
	st.global.f32 	[%rd60], %f104;

$L__BB0_16:
	ret;

}

`
	addexchangefourthorder_ptx_37 = `
.version 7.4
.target sm_37
.address_size 64

	// .globl	addexchangefourthorder

.visible .entry addexchangefourthorder(
	.param .u64 addexchangefourthorder_param_0,
	.param .u64 addexchangefourthorder_param_1,
	.param .u64 addexchangefourthorder_param_2,
	.param .u64 addexchangefourthorder_param_3,
	.param .u64 addexchangefourthorder_param_4,
	.param .u64 addexchangefourthorder_param_5,
	.param .u64 addexchangefourthorder_param_6,
	.param .f32 addexchangefourthorder_param_7,
	.param .u64 addexchangefourthorder_param_8,
	.param .u64 addexchangefourthorder_param_9,
	.param .u64 addexchangefourthorder_param_10,
	.param .f32 addexchangefourthorder_param_11,
	.param .f32 addexchangefourthorder_param_12,
	.param .f32 addexchangefourthorder_param_13,
	.param .u32 addexchangefourthorder_param_14,
	.param .u32 addexchangefourthorder_param_15,
	.param .u32 addexchangefourthorder_param_16,
	.param .u8 addexchangefourthorder_param_17
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<20>;
	.reg .f32 	%f<107>;
	.reg .b32 	%r<71>;
	.reg .b64 	%rd<61>;


	ld.param.u8 	%rs3, [addexchangefourthorder_param_17];
	ld.param.u64 	%rd7, [addexchangefourthorder_param_0];
	ld.param.u64 	%rd8, [addexchangefourthorder_param_1];
	ld.param.u64 	%rd9, [addexchangefourthorder_param_2];
	ld.param.u64 	%rd11, [addexchangefourthorder_param_3];
	ld.param.u64 	%rd12, [addexchangefourthorder_param_4];
	ld.param.u64 	%rd13, [addexchangefourthorder_param_5];
	ld.param.u64 	%rd10, [addexchangefourthorder_param_6];
	ld.param.f32 	%f105, [addexchangefourthorder_param_7];
	ld.param.u64 	%rd14, [addexchangefourthorder_param_8];
	ld.param.u64 	%rd15, [addexchangefourthorder_param_9];
	ld.param.u64 	%rd16, [addexchangefourthorder_param_10];
	ld.param.f32 	%f24, [addexchangefourthorder_param_11];
	ld.param.f32 	%f23, [addexchangefourthorder_param_12];
	ld.param.u32 	%r19, [addexchangefourthorder_param_14];
	ld.param.u32 	%r20, [addexchangefourthorder_param_15];
	ld.param.u32 	%r21, [addexchangefourthorder_param_16];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd15;
	cvta.to.global.u64 	%rd3, %rd16;
	cvta.to.global.u64 	%rd4, %rd13;
	cvta.to.global.u64 	%rd5, %rd12;
	cvta.to.global.u64 	%rd6, %rd11;
	mov.u32 	%r22, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r1, %r23, %r22, %r24;
	mov.u32 	%r25, %ntid.y;
	mov.u32 	%r26, %ctaid.y;
	mov.u32 	%r27, %tid.y;
	mad.lo.s32 	%r2, %r26, %r25, %r27;
	mov.u32 	%r28, %ntid.z;
	mov.u32 	%r29, %ctaid.z;
	mov.u32 	%r30, %tid.z;
	mad.lo.s32 	%r3, %r29, %r28, %r30;
	mul.f32 	%f25, %f24, %f24;
	mov.f32 	%f26, 0f40000000;
	div.rn.f32 	%f1, %f26, %f25;
	mul.f32 	%f27, %f25, %f24;
	mul.f32 	%f28, %f27, %f24;
	div.rn.f32 	%f2, %f26, %f28;
	setp.ge.s32 	%p1, %r1, %r19;
	setp.ge.s32 	%p2, %r2, %r20;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32 	%p4, %r3, %r21;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB0_16;

	mul.lo.s32 	%r4, %r3, %r20;
	add.s32 	%r31, %r4, %r2;
	mul.lo.s32 	%r5, %r31, %r19;
	add.s32 	%r6, %r5, %r1;
	mul.wide.s32 	%rd17, %r6, 4;
	add.s64 	%rd18, %rd6, %rd17;
	add.s64 	%rd19, %rd5, %rd17;
	add.s64 	%rd20, %rd4, %rd17;
	ld.global.nc.f32 	%f3, [%rd18];
	ld.global.nc.f32 	%f4, [%rd19];
	ld.global.nc.f32 	%f5, [%rd20];
	mul.f32 	%f29, %f4, %f4;
	fma.rn.f32 	%f30, %f3, %f3, %f29;
	fma.rn.f32 	%f31, %f5, %f5, %f30;
	setp.eq.f32 	%p6, %f31, 0f00000000;
	@%p6 bra 	$L__BB0_16;

	cvt.s64.s32 	%rd21, %r6;
	add.s64 	%rd22, %rd3, %rd21;
	ld.global.nc.u8 	%rs1, [%rd22];
	and.b16  	%rs2, %rs3, 1;
	setp.eq.s16 	%p7, %rs2, 0;
	add.s32 	%r7, %r1, -2;
	@%p7 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_3;

$L__BB0_4:
	max.s32 	%r68, %r7, 0;
	bra.uni 	$L__BB0_5;

$L__BB0_3:
	rem.s32 	%r32, %r7, %r19;
	add.s32 	%r33, %r32, %r19;
	rem.s32 	%r68, %r33, %r19;

$L__BB0_5:
	cvt.u32.u16 	%r34, %rs1;
	and.b32  	%r35, %r34, 255;
	add.s32 	%r36, %r68, %r5;
	cvt.s64.s32 	%rd23, %r36;
	mul.wide.s32 	%rd24, %r36, 4;
	add.s64 	%rd25, %rd6, %rd24;
	add.s64 	%rd26, %rd5, %rd24;
	add.s64 	%rd27, %rd4, %rd24;
	ld.global.nc.f32 	%f32, [%rd27];
	ld.global.nc.f32 	%f33, [%rd25];
	ld.global.nc.f32 	%f34, [%rd26];
	mul.f32 	%f35, %f34, %f34;
	fma.rn.f32 	%f36, %f33, %f33, %f35;
	fma.rn.f32 	%f37, %f32, %f32, %f36;
	setp.eq.f32 	%p8, %f37, 0f00000000;
	selp.f32 	%f38, %f5, %f32, %p8;
	selp.f32 	%f39, %f4, %f34, %p8;
	selp.f32 	%f40, %f3, %f33, %p8;
	add.s64 	%rd28, %rd3, %rd23;
	ld.global.nc.u8 	%rs5, [%rd28];
	min.u16 	%rs7, %rs5, %rs1;
	cvt.u32.u16 	%r37, %rs7;
	max.u16 	%rs8, %rs5, %rs1;
	cvt.u32.u16 	%r38, %rs8;
	add.s32 	%r39, %r38, 1;
	mul.lo.s32 	%r40, %r39, %r38;
	shr.u32 	%r41, %r40, 1;
	add.s32 	%r42, %r41, %r37;
	mul.wide.s32 	%rd29, %r42, 4;
	add.s64 	%rd30, %rd2, %rd29;
	ld.global.nc.f32 	%f41, [%rd30];
	mul.f32 	%f42, %f2, %f41;
	fma.rn.f32 	%f43, %f42, %f40, 0f00000000;
	fma.rn.f32 	%f44, %f42, %f39, 0f00000000;
	fma.rn.f32 	%f45, %f42, %f38, 0f00000000;
	add.s32 	%r43, %r35, 1;
	mul.lo.s32 	%r44, %r43, %r35;
	shr.u32 	%r45, %r44, 1;
	add.s32 	%r46, %r45, %r35;
	mul.wide.s32 	%rd31, %r46, 4;
	add.s64 	%rd32, %rd2, %rd31;
	mul.f32 	%f46, %f2, 0f40C00000;
	ld.global.nc.f32 	%f47, [%rd32];
	mul.f32 	%f48, %f46, %f47;
	fma.rn.f32 	%f49, %f48, %f3, %f43;
	fma.rn.f32 	%f50, %f48, %f4, %f44;
	fma.rn.f32 	%f51, %f48, %f5, %f45;
	add.s64 	%rd33, %rd1, %rd31;
	add.f32 	%f52, %f1, %f1;
	ld.global.nc.f32 	%f53, [%rd33];
	mul.f32 	%f54, %f52, %f53;
	mul.f32 	%f55, %f3, %f54;
	mul.f32 	%f56, %f4, %f54;
	mul.f32 	%f57, %f5, %f54;
	sub.f32 	%f9, %f49, %f55;
	sub.f32 	%f10, %f50, %f56;
	sub.f32 	%f11, %f51, %f57;
	add.s32 	%r11, %r1, 1;
	@%p7 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_6;

$L__BB0_7:
	add.s32 	%r49, %r19, -1;
	min.s32 	%r69, %r11, %r49;
	bra.uni 	$L__BB0_8;

$L__BB0_6:
	rem.s32 	%r47, %r11, %r19;
	add.s32 	%r48, %r47, %r19;
	rem.s32 	%r69, %r48, %r19;

$L__BB0_8:
	add.s32 	%r50, %r69, %r5;
	cvt.s64.s32 	%rd34, %r50;
	mul.wide.s32 	%rd35, %r50, 4;
	add.s64 	%rd36, %rd6, %rd35;
	add.s64 	%rd37, %rd5, %rd35;
	add.s64 	%rd38, %rd4, %rd35;
	ld.global.nc.f32 	%f58, [%rd38];
	ld.global.nc.f32 	%f59, [%rd36];
	ld.global.nc.f32 	%f60, [%rd37];
	mul.f32 	%f61, %f60, %f60;
	fma.rn.f32 	%f62, %f59, %f59, %f61;
	fma.rn.f32 	%f63, %f58, %f58, %f62;
	setp.eq.f32 	%p10, %f63, 0f00000000;
	selp.f32 	%f64, %f5, %f58, %p10;
	selp.f32 	%f65, %f4, %f60, %p10;
	selp.f32 	%f66, %f3, %f59, %p10;
	add.s64 	%rd39, %rd3, %rd34;
	ld.global.nc.u8 	%rs9, [%rd39];
	min.u16 	%rs12, %rs9, %rs1;
	cvt.u32.u16 	%r51, %rs12;
	max.u16 	%rs13, %rs9, %rs1;
	cvt.u32.u16 	%r52, %rs13;
	add.s32 	%r53, %r52, 1;
	mul.lo.s32 	%r54, %r53, %r52;
	shr.u32 	%r55, %r54, 1;
	add.s32 	%r56, %r55, %r51;
	mul.wide.s32 	%rd40, %r56, 4;
	add.s64 	%rd41, %rd2, %rd40;
	mul.f32 	%f67, %f2, 0f40800000;
	ld.global.nc.f32 	%f68, [%rd41];
	mul.f32 	%f69, %f67, %f68;
	mul.f32 	%f70, %f69, %f66;
	mul.f32 	%f71, %f69, %f65;
	mul.f32 	%f72, %f69, %f64;
	sub.f32 	%f73, %f9, %f70;
	sub.f32 	%f74, %f10, %f71;
	sub.f32 	%f75, %f11, %f72;
	add.s64 	%rd42, %rd1, %rd40;
	ld.global.nc.f32 	%f76, [%rd42];
	mul.f32 	%f77, %f1, %f76;
	fma.rn.f32 	%f12, %f66, %f77, %f73;
	fma.rn.f32 	%f13, %f65, %f77, %f74;
	fma.rn.f32 	%f14, %f64, %f77, %f75;
	and.b16  	%rs14, %rs3, 2;
	setp.eq.s16 	%p11, %rs14, 0;
	add.s32 	%r15, %r2, 2;
	@%p11 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_9;

$L__BB0_10:
	add.s32 	%r59, %r20, -1;
	min.s32 	%r70, %r15, %r59;
	bra.uni 	$L__BB0_11;

$L__BB0_9:
	rem.s32 	%r57, %r15, %r20;
	add.s32 	%r58, %r57, %r20;
	rem.s32 	%r70, %r58, %r20;

$L__BB0_11:
	add.s32 	%r60, %r70, %r4;
	mad.lo.s32 	%r61, %r60, %r19, %r1;
	cvt.s64.s32 	%rd43, %r61;
	mul.wide.s32 	%rd44, %r61, 4;
	add.s64 	%rd45, %rd6, %rd44;
	add.s64 	%rd46, %rd5, %rd44;
	add.s64 	%rd47, %rd4, %rd44;
	ld.global.nc.f32 	%f78, [%rd47];
	ld.global.nc.f32 	%f79, [%rd45];
	ld.global.nc.f32 	%f80, [%rd46];
	mul.f32 	%f81, %f80, %f80;
	fma.rn.f32 	%f82, %f79, %f79, %f81;
	fma.rn.f32 	%f83, %f78, %f78, %f82;
	setp.eq.f32 	%p12, %f83, 0f00000000;
	selp.f32 	%f84, %f5, %f78, %p12;
	selp.f32 	%f85, %f4, %f80, %p12;
	selp.f32 	%f86, %f3, %f79, %p12;
	add.s64 	%rd48, %rd3, %rd43;
	ld.global.nc.u8 	%rs15, [%rd48];
	min.u16 	%rs18, %rs15, %rs1;
	cvt.u32.u16 	%r62, %rs18;
	max.u16 	%rs19, %rs15, %rs1;
	cvt.u32.u16 	%r63, %rs19;
	add.s32 	%r64, %r63, 1;
	mul.lo.s32 	%r65, %r64, %r63;
	shr.u32 	%r66, %r65, 1;
	add.s32 	%r67, %r66, %r62;
	mul.wide.s32 	%rd49, %r67, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.nc.f32 	%f87, [%rd50];
	mul.f32 	%f88, %f23, %f23;
	mul.f32 	%f89, %f88, %f23;
	mul.f32 	%f90, %f89, %f23;
	div.rn.f32 	%f92, %f26, %f90;
	mul.f32 	%f93, %f92, %f87;
	fma.rn.f32 	%f15, %f93, %f86, %f12;
	fma.rn.f32 	%f16, %f93, %f85, %f13;
	fma.rn.f32 	%f17, %f93, %f84, %f14;
	setp.eq.s64 	%p13, %rd10, 0;
	@%p13 bra 	$L__BB0_13;

	cvta.to.global.u64 	%rd51, %rd10;
	add.s64 	%rd53, %rd51, %rd17;
	ld.global.nc.f32 	%f94, [%rd53];
	mul.f32 	%f105, %f94, %f105;

$L__BB0_13:
	setp.eq.f32 	%p14, %f105, 0f00000000;
	mov.f32 	%f106, 0f00000000;
	@%p14 bra 	$L__BB0_15;

	rcp.rn.f32 	%f106, %f105;

$L__BB0_15:
	cvta.to.global.u64 	%rd54, %rd7;
	add.s64 	%rd56, %rd54, %rd17;
	ld.global.f32 	%f96, [%rd56];
	mul.f32 	%f97, %f15, %f106;
	sub.f32 	%f98, %f96, %f97;
	st.global.f32 	[%rd56], %f98;
	cvta.to.global.u64 	%rd57, %rd8;
	add.s64 	%rd58, %rd57, %rd17;
	ld.global.f32 	%f99, [%rd58];
	mul.f32 	%f100, %f16, %f106;
	sub.f32 	%f101, %f99, %f100;
	st.global.f32 	[%rd58], %f101;
	cvta.to.global.u64 	%rd59, %rd9;
	add.s64 	%rd60, %rd59, %rd17;
	ld.global.f32 	%f102, [%rd60];
	mul.f32 	%f103, %f17, %f106;
	sub.f32 	%f104, %f102, %f103;
	st.global.f32 	[%rd60], %f104;

$L__BB0_16:
	ret;

}

`
	addexchangefourthorder_ptx_50 = `
.version 7.4
.target sm_50
.address_size 64

	// .globl	addexchangefourthorder

.visible .entry addexchangefourthorder(
	.param .u64 addexchangefourthorder_param_0,
	.param .u64 addexchangefourthorder_param_1,
	.param .u64 addexchangefourthorder_param_2,
	.param .u64 addexchangefourthorder_param_3,
	.param .u64 addexchangefourthorder_param_4,
	.param .u64 addexchangefourthorder_param_5,
	.param .u64 addexchangefourthorder_param_6,
	.param .f32 addexchangefourthorder_param_7,
	.param .u64 addexchangefourthorder_param_8,
	.param .u64 addexchangefourthorder_param_9,
	.param .u64 addexchangefourthorder_param_10,
	.param .f32 addexchangefourthorder_param_11,
	.param .f32 addexchangefourthorder_param_12,
	.param .f32 addexchangefourthorder_param_13,
	.param .u32 addexchangefourthorder_param_14,
	.param .u32 addexchangefourthorder_param_15,
	.param .u32 addexchangefourthorder_param_16,
	.param .u8 addexchangefourthorder_param_17
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<20>;
	.reg .f32 	%f<107>;
	.reg .b32 	%r<71>;
	.reg .b64 	%rd<61>;


	ld.param.u8 	%rs3, [addexchangefourthorder_param_17];
	ld.param.u64 	%rd7, [addexchangefourthorder_param_0];
	ld.param.u64 	%rd8, [addexchangefourthorder_param_1];
	ld.param.u64 	%rd9, [addexchangefourthorder_param_2];
	ld.param.u64 	%rd11, [addexchangefourthorder_param_3];
	ld.param.u64 	%rd12, [addexchangefourthorder_param_4];
	ld.param.u64 	%rd13, [addexchangefourthorder_param_5];
	ld.param.u64 	%rd10, [addexchangefourthorder_param_6];
	ld.param.f32 	%f105, [addexchangefourthorder_param_7];
	ld.param.u64 	%rd14, [addexchangefourthorder_param_8];
	ld.param.u64 	%rd15, [addexchangefourthorder_param_9];
	ld.param.u64 	%rd16, [addexchangefourthorder_param_10];
	ld.param.f32 	%f24, [addexchangefourthorder_param_11];
	ld.param.f32 	%f23, [addexchangefourthorder_param_12];
	ld.param.u32 	%r19, [addexchangefourthorder_param_14];
	ld.param.u32 	%r20, [addexchangefourthorder_param_15];
	ld.param.u32 	%r21, [addexchangefourthorder_param_16];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd15;
	cvta.to.global.u64 	%rd3, %rd16;
	cvta.to.global.u64 	%rd4, %rd13;
	cvta.to.global.u64 	%rd5, %rd12;
	cvta.to.global.u64 	%rd6, %rd11;
	mov.u32 	%r22, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r1, %r23, %r22, %r24;
	mov.u32 	%r25, %ntid.y;
	mov.u32 	%r26, %ctaid.y;
	mov.u32 	%r27, %tid.y;
	mad.lo.s32 	%r2, %r26, %r25, %r27;
	mov.u32 	%r28, %ntid.z;
	mov.u32 	%r29, %ctaid.z;
	mov.u32 	%r30, %tid.z;
	mad.lo.s32 	%r3, %r29, %r28, %r30;
	mul.f32 	%f25, %f24, %f24;
	mov.f32 	%f26, 0f40000000;
	div.rn.f32 	%f1, %f26, %f25;
	mul.f32 	%f27, %f25, %f24;
	mul.f32 	%f28, %f27, %f24;
	div.rn.f32 	%f2, %f26, %f28;
	setp.ge.s32 	%p1, %r1, %r19;
	setp.ge.s32 	%p2, %r2, %r20;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32 	%p4, %r3, %r21;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB0_16;

	mul.lo.s32 	%r4, %r3, %r20;
	add.s32 	%r31, %r4, %r2;
	mul.lo.s32 	%r5, %r31, %r19;
	add.s32 	%r6, %r5, %r1;
	mul.wide.s32 	%rd17, %r6, 4;
	add.s64 	%rd18, %rd6, %rd17;
	add.s64 	%rd19, %rd5, %rd17;
	add.s64 	%rd20, %rd4, %rd17;
	ld.global.nc.f32 	%f3, [%rd18];
	ld.global.nc.f32 	%f4, [%rd19];
	ld.global.nc.f32 	%f5, [%rd20];
	mul.f32 	%f29, %f4, %f4;
	fma.rn.f32 	%f30, %f3, %f3, %f29;
	fma.rn.f32 	%f31, %f5, %f5, %f30;
	setp.eq.f32 	%p6, %f31, 0f00000000;
	@%p6 bra 	$L__BB0_16;

	cvt.s64.s32 	%rd21, %r6;
	add.s64 	%rd22, %rd3, %rd21;
	ld.global.nc.u8 	%rs1, [%rd22];
	and.b16  	%rs2, %rs3, 1;
	setp.eq.s16 	%p7, %rs2, 0;
	add.s32 	%r7, %r1, -2;
	@%p7 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_3;

$L__BB0_4:
	max.s32 	%r68, %r7, 0;
	bra.uni 	$L__BB0_5;

$L__BB0_3:
	rem.s32 	%r32, %r7, %r19;
	add.s32 	%r33, %r32, %r19;
	rem.s32 	%r68, %r33, %r19;

$L__BB0_5:
	cvt.u32.u16 	%r34, %rs1;
	and.b32  	%r35, %r34, 255;
	add.s32 	%r36, %r68, %r5;
	cvt.s64.s32 	%rd23, %r36;
	mul.wide.s32 	%rd24, %r36, 4;
	add.s64 	%rd25, %rd6, %rd24;
	add.s64 	%rd26, %rd5, %rd24;
	add.s64 	%rd27, %rd4, %rd24;
	ld.global.nc.f32 	%f32, [%rd27];
	ld.global.nc.f32 	%f33, [%rd25];
	ld.global.nc.f32 	%f34, [%rd26];
	mul.f32 	%f35, %f34, %f34;
	fma.rn.f32 	%f36, %f33, %f33, %f35;
	fma.rn.f32 	%f37, %f32, %f32, %f36;
	setp.eq.f32 	%p8, %f37, 0f00000000;
	selp.f32 	%f38, %f5, %f32, %p8;
	selp.f32 	%f39, %f4, %f34, %p8;
	selp.f32 	%f40, %f3, %f33, %p8;
	add.s64 	%rd28, %rd3, %rd23;
	ld.global.nc.u8 	%rs5, [%rd28];
	min.u16 	%rs7, %rs5, %rs1;
	cvt.u32.u16 	%r37, %rs7;
	max.u16 	%rs8, %rs5, %rs1;
	cvt.u32.u16 	%r38, %rs8;
	add.s32 	%r39, %r38, 1;
	mul.lo.s32 	%r40, %r39, %r38;
	shr.u32 	%r41, %r40, 1;
	add.s32 	%r42, %r41, %r37;
	mul.wide.s32 	%rd29, %r42, 4;
	add.s64 	%rd30, %rd2, %rd29;
	ld.global.nc.f32 	%f41, [%rd30];
	mul.f32 	%f42, %f2, %f41;
	fma.rn.f32 	%f43, %f42, %f40, 0f00000000;
	fma.rn.f32 	%f44, %f42, %f39, 0f00000000;
	fma.rn.f32 	%f45, %f42, %f38, 0f00000000;
	add.s32 	%r43, %r35, 1;
	mul.lo.s32 	%r44, %r43, %r35;
	shr.u32 	%r45, %r44, 1;
	add.s32 	%r46, %r45, %r35;
	mul.wide.s32 	%rd31, %r46, 4;
	add.s64 	%rd32, %rd2, %rd31;
	mul.f32 	%f46, %f2, 0f40C00000;
	ld.global.nc.f32 	%f47, [%rd32];
	mul.f32 	%f48, %f46, %f47;
	fma.rn.f32 	%f49, %f48, %f3, %f43;
	fma.rn.f32 	%f50, %f48, %f4, %f44;
	fma.rn.f32 	%f51, %f48, %f5, %f45;
	add.s64 	%rd33, %rd1, %rd31;
	add.f32 	%f52, %f1, %f1;
	ld.global.nc.f32 	%f53, [%rd33];
	mul.f32 	%f54, %f52, %f53;
	mul.f32 	%f55, %f3, %f54;
	mul.f32 	%f56, %f4, %f54;
	mul.f32 	%f57, %f5, %f54;
	sub.f32 	%f9, %f49, %f55;
	sub.f32 	%f10, %f50, %f56;
	sub.f32 	%f11, %f51, %f57;
	add.s32 	%r11, %r1, 1;
	@%p7 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_6;

$L__BB0_7:
	add.s32 	%r49, %r19, -1;
	min.s32 	%r69, %r11, %r49;
	bra.uni 	$L__BB0_8;

$L__BB0_6:
	rem.s32 	%r47, %r11, %r19;
	add.s32 	%r48, %r47, %r19;
	rem.s32 	%r69, %r48, %r19;

$L__BB0_8:
	add.s32 	%r50, %r69, %r5;
	cvt.s64.s32 	%rd34, %r50;
	mul.wide.s32 	%rd35, %r50, 4;
	add.s64 	%rd36, %rd6, %rd35;
	add.s64 	%rd37, %rd5, %rd35;
	add.s64 	%rd38, %rd4, %rd35;
	ld.global.nc.f32 	%f58, [%rd38];
	ld.global.nc.f32 	%f59, [%rd36];
	ld.global.nc.f32 	%f60, [%rd37];
	mul.f32 	%f61, %f60, %f60;
	fma.rn.f32 	%f62, %f59, %f59, %f61;
	fma.rn.f32 	%f63, %f58, %f58, %f62;
	setp.eq.f32 	%p10, %f63, 0f00000000;
	selp.f32 	%f64, %f5, %f58, %p10;
	selp.f32 	%f65, %f4, %f60, %p10;
	selp.f32 	%f66, %f3, %f59, %p10;
	add.s64 	%rd39, %rd3, %rd34;
	ld.global.nc.u8 	%rs9, [%rd39];
	min.u16 	%rs12, %rs9, %rs1;
	cvt.u32.u16 	%r51, %rs12;
	max.u16 	%rs13, %rs9, %rs1;
	cvt.u32.u16 	%r52, %rs13;
	add.s32 	%r53, %r52, 1;
	mul.lo.s32 	%r54, %r53, %r52;
	shr.u32 	%r55, %r54, 1;
	add.s32 	%r56, %r55, %r51;
	mul.wide.s32 	%rd40, %r56, 4;
	add.s64 	%rd41, %rd2, %rd40;
	mul.f32 	%f67, %f2, 0f40800000;
	ld.global.nc.f32 	%f68, [%rd41];
	mul.f32 	%f69, %f67, %f68;
	mul.f32 	%f70, %f69, %f66;
	mul.f32 	%f71, %f69, %f65;
	mul.f32 	%f72, %f69, %f64;
	sub.f32 	%f73, %f9, %f70;
	sub.f32 	%f74, %f10, %f71;
	sub.f32 	%f75, %f11, %f72;
	add.s64 	%rd42, %rd1, %rd40;
	ld.global.nc.f32 	%f76, [%rd42];
	mul.f32 	%f77, %f1, %f76;
	fma.rn.f32 	%f12, %f66, %f77, %f73;
	fma.rn.f32 	%f13, %f65, %f77, %f74;
	fma.rn.f32 	%f14, %f64, %f77, %f75;
	and.b16  	%rs14, %rs3, 2;
	setp.eq.s16 	%p11, %rs14, 0;
	add.s32 	%r15, %r2, 2;
	@%p11 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_9;

$L__BB0_10:
	add.s32 	%r59, %r20, -1;
	min.s32 	%r70, %r15, %r59;
	bra.uni 	$L__BB0_11;

$L__BB0_9:
	rem.s32 	%r57, %r15, %r20;
	add.s32 	%r58, %r57, %r20;
	rem.s32 	%r70, %r58, %r20;

$L__BB0_11:
	add.s32 	%r60, %r70, %r4;
	mad.lo.s32 	%r61, %r60, %r19, %r1;
	cvt.s64.s32 	%rd43, %r61;
	mul.wide.s32 	%rd44, %r61, 4;
	add.s64 	%rd45, %rd6, %rd44;
	add.s64 	%rd46, %rd5, %rd44;
	add.s64 	%rd47, %rd4, %rd44;
	ld.global.nc.f32 	%f78, [%rd47];
	ld.global.nc.f32 	%f79, [%rd45];
	ld.global.nc.f32 	%f80, [%rd46];
	mul.f32 	%f81, %f80, %f80;
	fma.rn.f32 	%f82, %f79, %f79, %f81;
	fma.rn.f32 	%f83, %f78, %f78, %f82;
	setp.eq.f32 	%p12, %f83, 0f00000000;
	selp.f32 	%f84, %f5, %f78, %p12;
	selp.f32 	%f85, %f4, %f80, %p12;
	selp.f32 	%f86, %f3, %f79, %p12;
	add.s64 	%rd48, %rd3, %rd43;
	ld.global.nc.u8 	%rs15, [%rd48];
	min.u16 	%rs18, %rs15, %rs1;
	cvt.u32.u16 	%r62, %rs18;
	max.u16 	%rs19, %rs15, %rs1;
	cvt.u32.u16 	%r63, %rs19;
	add.s32 	%r64, %r63, 1;
	mul.lo.s32 	%r65, %r64, %r63;
	shr.u32 	%r66, %r65, 1;
	add.s32 	%r67, %r66, %r62;
	mul.wide.s32 	%rd49, %r67, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.nc.f32 	%f87, [%rd50];
	mul.f32 	%f88, %f23, %f23;
	mul.f32 	%f89, %f88, %f23;
	mul.f32 	%f90, %f89, %f23;
	div.rn.f32 	%f92, %f26, %f90;
	mul.f32 	%f93, %f92, %f87;
	fma.rn.f32 	%f15, %f93, %f86, %f12;
	fma.rn.f32 	%f16, %f93, %f85, %f13;
	fma.rn.f32 	%f17, %f93, %f84, %f14;
	setp.eq.s64 	%p13, %rd10, 0;
	@%p13 bra 	$L__BB0_13;

	cvta.to.global.u64 	%rd51, %rd10;
	add.s64 	%rd53, %rd51, %rd17;
	ld.global.nc.f32 	%f94, [%rd53];
	mul.f32 	%f105, %f94, %f105;

$L__BB0_13:
	setp.eq.f32 	%p14, %f105, 0f00000000;
	mov.f32 	%f106, 0f00000000;
	@%p14 bra 	$L__BB0_15;

	rcp.rn.f32 	%f106, %f105;

$L__BB0_15:
	cvta.to.global.u64 	%rd54, %rd7;
	add.s64 	%rd56, %rd54, %rd17;
	ld.global.f32 	%f96, [%rd56];
	mul.f32 	%f97, %f15, %f106;
	sub.f32 	%f98, %f96, %f97;
	st.global.f32 	[%rd56], %f98;
	cvta.to.global.u64 	%rd57, %rd8;
	add.s64 	%rd58, %rd57, %rd17;
	ld.global.f32 	%f99, [%rd58];
	mul.f32 	%f100, %f16, %f106;
	sub.f32 	%f101, %f99, %f100;
	st.global.f32 	[%rd58], %f101;
	cvta.to.global.u64 	%rd59, %rd9;
	add.s64 	%rd60, %rd59, %rd17;
	ld.global.f32 	%f102, [%rd60];
	mul.f32 	%f103, %f17, %f106;
	sub.f32 	%f104, %f102, %f103;
	st.global.f32 	[%rd60], %f104;

$L__BB0_16:
	ret;

}

`
	addexchangefourthorder_ptx_52 = `
.version 7.4
.target sm_52
.address_size 64

	// .globl	addexchangefourthorder

.visible .entry addexchangefourthorder(
	.param .u64 addexchangefourthorder_param_0,
	.param .u64 addexchangefourthorder_param_1,
	.param .u64 addexchangefourthorder_param_2,
	.param .u64 addexchangefourthorder_param_3,
	.param .u64 addexchangefourthorder_param_4,
	.param .u64 addexchangefourthorder_param_5,
	.param .u64 addexchangefourthorder_param_6,
	.param .f32 addexchangefourthorder_param_7,
	.param .u64 addexchangefourthorder_param_8,
	.param .u64 addexchangefourthorder_param_9,
	.param .u64 addexchangefourthorder_param_10,
	.param .f32 addexchangefourthorder_param_11,
	.param .f32 addexchangefourthorder_param_12,
	.param .f32 addexchangefourthorder_param_13,
	.param .u32 addexchangefourthorder_param_14,
	.param .u32 addexchangefourthorder_param_15,
	.param .u32 addexchangefourthorder_param_16,
	.param .u8 addexchangefourthorder_param_17
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<20>;
	.reg .f32 	%f<107>;
	.reg .b32 	%r<71>;
	.reg .b64 	%rd<61>;


	ld.param.u8 	%rs3, [addexchangefourthorder_param_17];
	ld.param.u64 	%rd7, [addexchangefourthorder_param_0];
	ld.param.u64 	%rd8, [addexchangefourthorder_param_1];
	ld.param.u64 	%rd9, [addexchangefourthorder_param_2];
	ld.param.u64 	%rd11, [addexchangefourthorder_param_3];
	ld.param.u64 	%rd12, [addexchangefourthorder_param_4];
	ld.param.u64 	%rd13, [addexchangefourthorder_param_5];
	ld.param.u64 	%rd10, [addexchangefourthorder_param_6];
	ld.param.f32 	%f105, [addexchangefourthorder_param_7];
	ld.param.u64 	%rd14, [addexchangefourthorder_param_8];
	ld.param.u64 	%rd15, [addexchangefourthorder_param_9];
	ld.param.u64 	%rd16, [addexchangefourthorder_param_10];
	ld.param.f32 	%f24, [addexchangefourthorder_param_11];
	ld.param.f32 	%f23, [addexchangefourthorder_param_12];
	ld.param.u32 	%r19, [addexchangefourthorder_param_14];
	ld.param.u32 	%r20, [addexchangefourthorder_param_15];
	ld.param.u32 	%r21, [addexchangefourthorder_param_16];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd15;
	cvta.to.global.u64 	%rd3, %rd16;
	cvta.to.global.u64 	%rd4, %rd13;
	cvta.to.global.u64 	%rd5, %rd12;
	cvta.to.global.u64 	%rd6, %rd11;
	mov.u32 	%r22, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r1, %r23, %r22, %r24;
	mov.u32 	%r25, %ntid.y;
	mov.u32 	%r26, %ctaid.y;
	mov.u32 	%r27, %tid.y;
	mad.lo.s32 	%r2, %r26, %r25, %r27;
	mov.u32 	%r28, %ntid.z;
	mov.u32 	%r29, %ctaid.z;
	mov.u32 	%r30, %tid.z;
	mad.lo.s32 	%r3, %r29, %r28, %r30;
	mul.f32 	%f25, %f24, %f24;
	mov.f32 	%f26, 0f40000000;
	div.rn.f32 	%f1, %f26, %f25;
	mul.f32 	%f27, %f25, %f24;
	mul.f32 	%f28, %f27, %f24;
	div.rn.f32 	%f2, %f26, %f28;
	setp.ge.s32 	%p1, %r1, %r19;
	setp.ge.s32 	%p2, %r2, %r20;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32 	%p4, %r3, %r21;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB0_16;

	mul.lo.s32 	%r4, %r3, %r20;
	add.s32 	%r31, %r4, %r2;
	mul.lo.s32 	%r5, %r31, %r19;
	add.s32 	%r6, %r5, %r1;
	mul.wide.s32 	%rd17, %r6, 4;
	add.s64 	%rd18, %rd6, %rd17;
	add.s64 	%rd19, %rd5, %rd17;
	add.s64 	%rd20, %rd4, %rd17;
	ld.global.nc.f32 	%f3, [%rd18];
	ld.global.nc.f32 	%f4, [%rd19];
	ld.global.nc.f32 	%f5, [%rd20];
	mul.f32 	%f29, %f4, %f4;
	fma.rn.f32 	%f30, %f3, %f3, %f29;
	fma.rn.f32 	%f31, %f5, %f5, %f30;
	setp.eq.f32 	%p6, %f31, 0f00000000;
	@%p6 bra 	$L__BB0_16;

	cvt.s64.s32 	%rd21, %r6;
	add.s64 	%rd22, %rd3, %rd21;
	ld.global.nc.u8 	%rs1, [%rd22];
	and.b16  	%rs2, %rs3, 1;
	setp.eq.s16 	%p7, %rs2, 0;
	add.s32 	%r7, %r1, -2;
	@%p7 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_3;

$L__BB0_4:
	max.s32 	%r68, %r7, 0;
	bra.uni 	$L__BB0_5;

$L__BB0_3:
	rem.s32 	%r32, %r7, %r19;
	add.s32 	%r33, %r32, %r19;
	rem.s32 	%r68, %r33, %r19;

$L__BB0_5:
	cvt.u32.u16 	%r34, %rs1;
	and.b32  	%r35, %r34, 255;
	add.s32 	%r36, %r68, %r5;
	cvt.s64.s32 	%rd23, %r36;
	mul.wide.s32 	%rd24, %r36, 4;
	add.s64 	%rd25, %rd6, %rd24;
	add.s64 	%rd26, %rd5, %rd24;
	add.s64 	%rd27, %rd4, %rd24;
	ld.global.nc.f32 	%f32, [%rd27];
	ld.global.nc.f32 	%f33, [%rd25];
	ld.global.nc.f32 	%f34, [%rd26];
	mul.f32 	%f35, %f34, %f34;
	fma.rn.f32 	%f36, %f33, %f33, %f35;
	fma.rn.f32 	%f37, %f32, %f32, %f36;
	setp.eq.f32 	%p8, %f37, 0f00000000;
	selp.f32 	%f38, %f5, %f32, %p8;
	selp.f32 	%f39, %f4, %f34, %p8;
	selp.f32 	%f40, %f3, %f33, %p8;
	add.s64 	%rd28, %rd3, %rd23;
	ld.global.nc.u8 	%rs5, [%rd28];
	min.u16 	%rs7, %rs5, %rs1;
	cvt.u32.u16 	%r37, %rs7;
	max.u16 	%rs8, %rs5, %rs1;
	cvt.u32.u16 	%r38, %rs8;
	add.s32 	%r39, %r38, 1;
	mul.lo.s32 	%r40, %r39, %r38;
	shr.u32 	%r41, %r40, 1;
	add.s32 	%r42, %r41, %r37;
	mul.wide.s32 	%rd29, %r42, 4;
	add.s64 	%rd30, %rd2, %rd29;
	ld.global.nc.f32 	%f41, [%rd30];
	mul.f32 	%f42, %f2, %f41;
	fma.rn.f32 	%f43, %f42, %f40, 0f00000000;
	fma.rn.f32 	%f44, %f42, %f39, 0f00000000;
	fma.rn.f32 	%f45, %f42, %f38, 0f00000000;
	add.s32 	%r43, %r35, 1;
	mul.lo.s32 	%r44, %r43, %r35;
	shr.u32 	%r45, %r44, 1;
	add.s32 	%r46, %r45, %r35;
	mul.wide.s32 	%rd31, %r46, 4;
	add.s64 	%rd32, %rd2, %rd31;
	mul.f32 	%f46, %f2, 0f40C00000;
	ld.global.nc.f32 	%f47, [%rd32];
	mul.f32 	%f48, %f46, %f47;
	fma.rn.f32 	%f49, %f48, %f3, %f43;
	fma.rn.f32 	%f50, %f48, %f4, %f44;
	fma.rn.f32 	%f51, %f48, %f5, %f45;
	add.s64 	%rd33, %rd1, %rd31;
	add.f32 	%f52, %f1, %f1;
	ld.global.nc.f32 	%f53, [%rd33];
	mul.f32 	%f54, %f52, %f53;
	mul.f32 	%f55, %f3, %f54;
	mul.f32 	%f56, %f4, %f54;
	mul.f32 	%f57, %f5, %f54;
	sub.f32 	%f9, %f49, %f55;
	sub.f32 	%f10, %f50, %f56;
	sub.f32 	%f11, %f51, %f57;
	add.s32 	%r11, %r1, 1;
	@%p7 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_6;

$L__BB0_7:
	add.s32 	%r49, %r19, -1;
	min.s32 	%r69, %r11, %r49;
	bra.uni 	$L__BB0_8;

$L__BB0_6:
	rem.s32 	%r47, %r11, %r19;
	add.s32 	%r48, %r47, %r19;
	rem.s32 	%r69, %r48, %r19;

$L__BB0_8:
	add.s32 	%r50, %r69, %r5;
	cvt.s64.s32 	%rd34, %r50;
	mul.wide.s32 	%rd35, %r50, 4;
	add.s64 	%rd36, %rd6, %rd35;
	add.s64 	%rd37, %rd5, %rd35;
	add.s64 	%rd38, %rd4, %rd35;
	ld.global.nc.f32 	%f58, [%rd38];
	ld.global.nc.f32 	%f59, [%rd36];
	ld.global.nc.f32 	%f60, [%rd37];
	mul.f32 	%f61, %f60, %f60;
	fma.rn.f32 	%f62, %f59, %f59, %f61;
	fma.rn.f32 	%f63, %f58, %f58, %f62;
	setp.eq.f32 	%p10, %f63, 0f00000000;
	selp.f32 	%f64, %f5, %f58, %p10;
	selp.f32 	%f65, %f4, %f60, %p10;
	selp.f32 	%f66, %f3, %f59, %p10;
	add.s64 	%rd39, %rd3, %rd34;
	ld.global.nc.u8 	%rs9, [%rd39];
	min.u16 	%rs12, %rs9, %rs1;
	cvt.u32.u16 	%r51, %rs12;
	max.u16 	%rs13, %rs9, %rs1;
	cvt.u32.u16 	%r52, %rs13;
	add.s32 	%r53, %r52, 1;
	mul.lo.s32 	%r54, %r53, %r52;
	shr.u32 	%r55, %r54, 1;
	add.s32 	%r56, %r55, %r51;
	mul.wide.s32 	%rd40, %r56, 4;
	add.s64 	%rd41, %rd2, %rd40;
	mul.f32 	%f67, %f2, 0f40800000;
	ld.global.nc.f32 	%f68, [%rd41];
	mul.f32 	%f69, %f67, %f68;
	mul.f32 	%f70, %f69, %f66;
	mul.f32 	%f71, %f69, %f65;
	mul.f32 	%f72, %f69, %f64;
	sub.f32 	%f73, %f9, %f70;
	sub.f32 	%f74, %f10, %f71;
	sub.f32 	%f75, %f11, %f72;
	add.s64 	%rd42, %rd1, %rd40;
	ld.global.nc.f32 	%f76, [%rd42];
	mul.f32 	%f77, %f1, %f76;
	fma.rn.f32 	%f12, %f66, %f77, %f73;
	fma.rn.f32 	%f13, %f65, %f77, %f74;
	fma.rn.f32 	%f14, %f64, %f77, %f75;
	and.b16  	%rs14, %rs3, 2;
	setp.eq.s16 	%p11, %rs14, 0;
	add.s32 	%r15, %r2, 2;
	@%p11 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_9;

$L__BB0_10:
	add.s32 	%r59, %r20, -1;
	min.s32 	%r70, %r15, %r59;
	bra.uni 	$L__BB0_11;

$L__BB0_9:
	rem.s32 	%r57, %r15, %r20;
	add.s32 	%r58, %r57, %r20;
	rem.s32 	%r70, %r58, %r20;

$L__BB0_11:
	add.s32 	%r60, %r70, %r4;
	mad.lo.s32 	%r61, %r60, %r19, %r1;
	cvt.s64.s32 	%rd43, %r61;
	mul.wide.s32 	%rd44, %r61, 4;
	add.s64 	%rd45, %rd6, %rd44;
	add.s64 	%rd46, %rd5, %rd44;
	add.s64 	%rd47, %rd4, %rd44;
	ld.global.nc.f32 	%f78, [%rd47];
	ld.global.nc.f32 	%f79, [%rd45];
	ld.global.nc.f32 	%f80, [%rd46];
	mul.f32 	%f81, %f80, %f80;
	fma.rn.f32 	%f82, %f79, %f79, %f81;
	fma.rn.f32 	%f83, %f78, %f78, %f82;
	setp.eq.f32 	%p12, %f83, 0f00000000;
	selp.f32 	%f84, %f5, %f78, %p12;
	selp.f32 	%f85, %f4, %f80, %p12;
	selp.f32 	%f86, %f3, %f79, %p12;
	add.s64 	%rd48, %rd3, %rd43;
	ld.global.nc.u8 	%rs15, [%rd48];
	min.u16 	%rs18, %rs15, %rs1;
	cvt.u32.u16 	%r62, %rs18;
	max.u16 	%rs19, %rs15, %rs1;
	cvt.u32.u16 	%r63, %rs19;
	add.s32 	%r64, %r63, 1;
	mul.lo.s32 	%r65, %r64, %r63;
	shr.u32 	%r66, %r65, 1;
	add.s32 	%r67, %r66, %r62;
	mul.wide.s32 	%rd49, %r67, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.nc.f32 	%f87, [%rd50];
	mul.f32 	%f88, %f23, %f23;
	mul.f32 	%f89, %f88, %f23;
	mul.f32 	%f90, %f89, %f23;
	div.rn.f32 	%f92, %f26, %f90;
	mul.f32 	%f93, %f92, %f87;
	fma.rn.f32 	%f15, %f93, %f86, %f12;
	fma.rn.f32 	%f16, %f93, %f85, %f13;
	fma.rn.f32 	%f17, %f93, %f84, %f14;
	setp.eq.s64 	%p13, %rd10, 0;
	@%p13 bra 	$L__BB0_13;

	cvta.to.global.u64 	%rd51, %rd10;
	add.s64 	%rd53, %rd51, %rd17;
	ld.global.nc.f32 	%f94, [%rd53];
	mul.f32 	%f105, %f94, %f105;

$L__BB0_13:
	setp.eq.f32 	%p14, %f105, 0f00000000;
	mov.f32 	%f106, 0f00000000;
	@%p14 bra 	$L__BB0_15;

	rcp.rn.f32 	%f106, %f105;

$L__BB0_15:
	cvta.to.global.u64 	%rd54, %rd7;
	add.s64 	%rd56, %rd54, %rd17;
	ld.global.f32 	%f96, [%rd56];
	mul.f32 	%f97, %f15, %f106;
	sub.f32 	%f98, %f96, %f97;
	st.global.f32 	[%rd56], %f98;
	cvta.to.global.u64 	%rd57, %rd8;
	add.s64 	%rd58, %rd57, %rd17;
	ld.global.f32 	%f99, [%rd58];
	mul.f32 	%f100, %f16, %f106;
	sub.f32 	%f101, %f99, %f100;
	st.global.f32 	[%rd58], %f101;
	cvta.to.global.u64 	%rd59, %rd9;
	add.s64 	%rd60, %rd59, %rd17;
	ld.global.f32 	%f102, [%rd60];
	mul.f32 	%f103, %f17, %f106;
	sub.f32 	%f104, %f102, %f103;
	st.global.f32 	[%rd60], %f104;

$L__BB0_16:
	ret;

}

`
	addexchangefourthorder_ptx_53 = `
.version 7.4
.target sm_53
.address_size 64

	// .globl	addexchangefourthorder

.visible .entry addexchangefourthorder(
	.param .u64 addexchangefourthorder_param_0,
	.param .u64 addexchangefourthorder_param_1,
	.param .u64 addexchangefourthorder_param_2,
	.param .u64 addexchangefourthorder_param_3,
	.param .u64 addexchangefourthorder_param_4,
	.param .u64 addexchangefourthorder_param_5,
	.param .u64 addexchangefourthorder_param_6,
	.param .f32 addexchangefourthorder_param_7,
	.param .u64 addexchangefourthorder_param_8,
	.param .u64 addexchangefourthorder_param_9,
	.param .u64 addexchangefourthorder_param_10,
	.param .f32 addexchangefourthorder_param_11,
	.param .f32 addexchangefourthorder_param_12,
	.param .f32 addexchangefourthorder_param_13,
	.param .u32 addexchangefourthorder_param_14,
	.param .u32 addexchangefourthorder_param_15,
	.param .u32 addexchangefourthorder_param_16,
	.param .u8 addexchangefourthorder_param_17
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<20>;
	.reg .f32 	%f<107>;
	.reg .b32 	%r<71>;
	.reg .b64 	%rd<61>;


	ld.param.u8 	%rs3, [addexchangefourthorder_param_17];
	ld.param.u64 	%rd7, [addexchangefourthorder_param_0];
	ld.param.u64 	%rd8, [addexchangefourthorder_param_1];
	ld.param.u64 	%rd9, [addexchangefourthorder_param_2];
	ld.param.u64 	%rd11, [addexchangefourthorder_param_3];
	ld.param.u64 	%rd12, [addexchangefourthorder_param_4];
	ld.param.u64 	%rd13, [addexchangefourthorder_param_5];
	ld.param.u64 	%rd10, [addexchangefourthorder_param_6];
	ld.param.f32 	%f105, [addexchangefourthorder_param_7];
	ld.param.u64 	%rd14, [addexchangefourthorder_param_8];
	ld.param.u64 	%rd15, [addexchangefourthorder_param_9];
	ld.param.u64 	%rd16, [addexchangefourthorder_param_10];
	ld.param.f32 	%f24, [addexchangefourthorder_param_11];
	ld.param.f32 	%f23, [addexchangefourthorder_param_12];
	ld.param.u32 	%r19, [addexchangefourthorder_param_14];
	ld.param.u32 	%r20, [addexchangefourthorder_param_15];
	ld.param.u32 	%r21, [addexchangefourthorder_param_16];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd15;
	cvta.to.global.u64 	%rd3, %rd16;
	cvta.to.global.u64 	%rd4, %rd13;
	cvta.to.global.u64 	%rd5, %rd12;
	cvta.to.global.u64 	%rd6, %rd11;
	mov.u32 	%r22, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r1, %r23, %r22, %r24;
	mov.u32 	%r25, %ntid.y;
	mov.u32 	%r26, %ctaid.y;
	mov.u32 	%r27, %tid.y;
	mad.lo.s32 	%r2, %r26, %r25, %r27;
	mov.u32 	%r28, %ntid.z;
	mov.u32 	%r29, %ctaid.z;
	mov.u32 	%r30, %tid.z;
	mad.lo.s32 	%r3, %r29, %r28, %r30;
	mul.f32 	%f25, %f24, %f24;
	mov.f32 	%f26, 0f40000000;
	div.rn.f32 	%f1, %f26, %f25;
	mul.f32 	%f27, %f25, %f24;
	mul.f32 	%f28, %f27, %f24;
	div.rn.f32 	%f2, %f26, %f28;
	setp.ge.s32 	%p1, %r1, %r19;
	setp.ge.s32 	%p2, %r2, %r20;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32 	%p4, %r3, %r21;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB0_16;

	mul.lo.s32 	%r4, %r3, %r20;
	add.s32 	%r31, %r4, %r2;
	mul.lo.s32 	%r5, %r31, %r19;
	add.s32 	%r6, %r5, %r1;
	mul.wide.s32 	%rd17, %r6, 4;
	add.s64 	%rd18, %rd6, %rd17;
	add.s64 	%rd19, %rd5, %rd17;
	add.s64 	%rd20, %rd4, %rd17;
	ld.global.nc.f32 	%f3, [%rd18];
	ld.global.nc.f32 	%f4, [%rd19];
	ld.global.nc.f32 	%f5, [%rd20];
	mul.f32 	%f29, %f4, %f4;
	fma.rn.f32 	%f30, %f3, %f3, %f29;
	fma.rn.f32 	%f31, %f5, %f5, %f30;
	setp.eq.f32 	%p6, %f31, 0f00000000;
	@%p6 bra 	$L__BB0_16;

	cvt.s64.s32 	%rd21, %r6;
	add.s64 	%rd22, %rd3, %rd21;
	ld.global.nc.u8 	%rs1, [%rd22];
	and.b16  	%rs2, %rs3, 1;
	setp.eq.s16 	%p7, %rs2, 0;
	add.s32 	%r7, %r1, -2;
	@%p7 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_3;

$L__BB0_4:
	max.s32 	%r68, %r7, 0;
	bra.uni 	$L__BB0_5;

$L__BB0_3:
	rem.s32 	%r32, %r7, %r19;
	add.s32 	%r33, %r32, %r19;
	rem.s32 	%r68, %r33, %r19;

$L__BB0_5:
	cvt.u32.u16 	%r34, %rs1;
	and.b32  	%r35, %r34, 255;
	add.s32 	%r36, %r68, %r5;
	cvt.s64.s32 	%rd23, %r36;
	mul.wide.s32 	%rd24, %r36, 4;
	add.s64 	%rd25, %rd6, %rd24;
	add.s64 	%rd26, %rd5, %rd24;
	add.s64 	%rd27, %rd4, %rd24;
	ld.global.nc.f32 	%f32, [%rd27];
	ld.global.nc.f32 	%f33, [%rd25];
	ld.global.nc.f32 	%f34, [%rd26];
	mul.f32 	%f35, %f34, %f34;
	fma.rn.f32 	%f36, %f33, %f33, %f35;
	fma.rn.f32 	%f37, %f32, %f32, %f36;
	setp.eq.f32 	%p8, %f37, 0f00000000;
	selp.f32 	%f38, %f5, %f32, %p8;
	selp.f32 	%f39, %f4, %f34, %p8;
	selp.f32 	%f40, %f3, %f33, %p8;
	add.s64 	%rd28, %rd3, %rd23;
	ld.global.nc.u8 	%rs5, [%rd28];
	min.u16 	%rs7, %rs5, %rs1;
	cvt.u32.u16 	%r37, %rs7;
	max.u16 	%rs8, %rs5, %rs1;
	cvt.u32.u16 	%r38, %rs8;
	add.s32 	%r39, %r38, 1;
	mul.lo.s32 	%r40, %r39, %r38;
	shr.u32 	%r41, %r40, 1;
	add.s32 	%r42, %r41, %r37;
	mul.wide.s32 	%rd29, %r42, 4;
	add.s64 	%rd30, %rd2, %rd29;
	ld.global.nc.f32 	%f41, [%rd30];
	mul.f32 	%f42, %f2, %f41;
	fma.rn.f32 	%f43, %f42, %f40, 0f00000000;
	fma.rn.f32 	%f44, %f42, %f39, 0f00000000;
	fma.rn.f32 	%f45, %f42, %f38, 0f00000000;
	add.s32 	%r43, %r35, 1;
	mul.lo.s32 	%r44, %r43, %r35;
	shr.u32 	%r45, %r44, 1;
	add.s32 	%r46, %r45, %r35;
	mul.wide.s32 	%rd31, %r46, 4;
	add.s64 	%rd32, %rd2, %rd31;
	mul.f32 	%f46, %f2, 0f40C00000;
	ld.global.nc.f32 	%f47, [%rd32];
	mul.f32 	%f48, %f46, %f47;
	fma.rn.f32 	%f49, %f48, %f3, %f43;
	fma.rn.f32 	%f50, %f48, %f4, %f44;
	fma.rn.f32 	%f51, %f48, %f5, %f45;
	add.s64 	%rd33, %rd1, %rd31;
	add.f32 	%f52, %f1, %f1;
	ld.global.nc.f32 	%f53, [%rd33];
	mul.f32 	%f54, %f52, %f53;
	mul.f32 	%f55, %f3, %f54;
	mul.f32 	%f56, %f4, %f54;
	mul.f32 	%f57, %f5, %f54;
	sub.f32 	%f9, %f49, %f55;
	sub.f32 	%f10, %f50, %f56;
	sub.f32 	%f11, %f51, %f57;
	add.s32 	%r11, %r1, 1;
	@%p7 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_6;

$L__BB0_7:
	add.s32 	%r49, %r19, -1;
	min.s32 	%r69, %r11, %r49;
	bra.uni 	$L__BB0_8;

$L__BB0_6:
	rem.s32 	%r47, %r11, %r19;
	add.s32 	%r48, %r47, %r19;
	rem.s32 	%r69, %r48, %r19;

$L__BB0_8:
	add.s32 	%r50, %r69, %r5;
	cvt.s64.s32 	%rd34, %r50;
	mul.wide.s32 	%rd35, %r50, 4;
	add.s64 	%rd36, %rd6, %rd35;
	add.s64 	%rd37, %rd5, %rd35;
	add.s64 	%rd38, %rd4, %rd35;
	ld.global.nc.f32 	%f58, [%rd38];
	ld.global.nc.f32 	%f59, [%rd36];
	ld.global.nc.f32 	%f60, [%rd37];
	mul.f32 	%f61, %f60, %f60;
	fma.rn.f32 	%f62, %f59, %f59, %f61;
	fma.rn.f32 	%f63, %f58, %f58, %f62;
	setp.eq.f32 	%p10, %f63, 0f00000000;
	selp.f32 	%f64, %f5, %f58, %p10;
	selp.f32 	%f65, %f4, %f60, %p10;
	selp.f32 	%f66, %f3, %f59, %p10;
	add.s64 	%rd39, %rd3, %rd34;
	ld.global.nc.u8 	%rs9, [%rd39];
	min.u16 	%rs12, %rs9, %rs1;
	cvt.u32.u16 	%r51, %rs12;
	max.u16 	%rs13, %rs9, %rs1;
	cvt.u32.u16 	%r52, %rs13;
	add.s32 	%r53, %r52, 1;
	mul.lo.s32 	%r54, %r53, %r52;
	shr.u32 	%r55, %r54, 1;
	add.s32 	%r56, %r55, %r51;
	mul.wide.s32 	%rd40, %r56, 4;
	add.s64 	%rd41, %rd2, %rd40;
	mul.f32 	%f67, %f2, 0f40800000;
	ld.global.nc.f32 	%f68, [%rd41];
	mul.f32 	%f69, %f67, %f68;
	mul.f32 	%f70, %f69, %f66;
	mul.f32 	%f71, %f69, %f65;
	mul.f32 	%f72, %f69, %f64;
	sub.f32 	%f73, %f9, %f70;
	sub.f32 	%f74, %f10, %f71;
	sub.f32 	%f75, %f11, %f72;
	add.s64 	%rd42, %rd1, %rd40;
	ld.global.nc.f32 	%f76, [%rd42];
	mul.f32 	%f77, %f1, %f76;
	fma.rn.f32 	%f12, %f66, %f77, %f73;
	fma.rn.f32 	%f13, %f65, %f77, %f74;
	fma.rn.f32 	%f14, %f64, %f77, %f75;
	and.b16  	%rs14, %rs3, 2;
	setp.eq.s16 	%p11, %rs14, 0;
	add.s32 	%r15, %r2, 2;
	@%p11 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_9;

$L__BB0_10:
	add.s32 	%r59, %r20, -1;
	min.s32 	%r70, %r15, %r59;
	bra.uni 	$L__BB0_11;

$L__BB0_9:
	rem.s32 	%r57, %r15, %r20;
	add.s32 	%r58, %r57, %r20;
	rem.s32 	%r70, %r58, %r20;

$L__BB0_11:
	add.s32 	%r60, %r70, %r4;
	mad.lo.s32 	%r61, %r60, %r19, %r1;
	cvt.s64.s32 	%rd43, %r61;
	mul.wide.s32 	%rd44, %r61, 4;
	add.s64 	%rd45, %rd6, %rd44;
	add.s64 	%rd46, %rd5, %rd44;
	add.s64 	%rd47, %rd4, %rd44;
	ld.global.nc.f32 	%f78, [%rd47];
	ld.global.nc.f32 	%f79, [%rd45];
	ld.global.nc.f32 	%f80, [%rd46];
	mul.f32 	%f81, %f80, %f80;
	fma.rn.f32 	%f82, %f79, %f79, %f81;
	fma.rn.f32 	%f83, %f78, %f78, %f82;
	setp.eq.f32 	%p12, %f83, 0f00000000;
	selp.f32 	%f84, %f5, %f78, %p12;
	selp.f32 	%f85, %f4, %f80, %p12;
	selp.f32 	%f86, %f3, %f79, %p12;
	add.s64 	%rd48, %rd3, %rd43;
	ld.global.nc.u8 	%rs15, [%rd48];
	min.u16 	%rs18, %rs15, %rs1;
	cvt.u32.u16 	%r62, %rs18;
	max.u16 	%rs19, %rs15, %rs1;
	cvt.u32.u16 	%r63, %rs19;
	add.s32 	%r64, %r63, 1;
	mul.lo.s32 	%r65, %r64, %r63;
	shr.u32 	%r66, %r65, 1;
	add.s32 	%r67, %r66, %r62;
	mul.wide.s32 	%rd49, %r67, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.nc.f32 	%f87, [%rd50];
	mul.f32 	%f88, %f23, %f23;
	mul.f32 	%f89, %f88, %f23;
	mul.f32 	%f90, %f89, %f23;
	div.rn.f32 	%f92, %f26, %f90;
	mul.f32 	%f93, %f92, %f87;
	fma.rn.f32 	%f15, %f93, %f86, %f12;
	fma.rn.f32 	%f16, %f93, %f85, %f13;
	fma.rn.f32 	%f17, %f93, %f84, %f14;
	setp.eq.s64 	%p13, %rd10, 0;
	@%p13 bra 	$L__BB0_13;

	cvta.to.global.u64 	%rd51, %rd10;
	add.s64 	%rd53, %rd51, %rd17;
	ld.global.nc.f32 	%f94, [%rd53];
	mul.f32 	%f105, %f94, %f105;

$L__BB0_13:
	setp.eq.f32 	%p14, %f105, 0f00000000;
	mov.f32 	%f106, 0f00000000;
	@%p14 bra 	$L__BB0_15;

	rcp.rn.f32 	%f106, %f105;

$L__BB0_15:
	cvta.to.global.u64 	%rd54, %rd7;
	add.s64 	%rd56, %rd54, %rd17;
	ld.global.f32 	%f96, [%rd56];
	mul.f32 	%f97, %f15, %f106;
	sub.f32 	%f98, %f96, %f97;
	st.global.f32 	[%rd56], %f98;
	cvta.to.global.u64 	%rd57, %rd8;
	add.s64 	%rd58, %rd57, %rd17;
	ld.global.f32 	%f99, [%rd58];
	mul.f32 	%f100, %f16, %f106;
	sub.f32 	%f101, %f99, %f100;
	st.global.f32 	[%rd58], %f101;
	cvta.to.global.u64 	%rd59, %rd9;
	add.s64 	%rd60, %rd59, %rd17;
	ld.global.f32 	%f102, [%rd60];
	mul.f32 	%f103, %f17, %f106;
	sub.f32 	%f104, %f102, %f103;
	st.global.f32 	[%rd60], %f104;

$L__BB0_16:
	ret;

}

`
	addexchangefourthorder_ptx_60 = `
.version 7.4
.target sm_60
.address_size 64

	// .globl	addexchangefourthorder

.visible .entry addexchangefourthorder(
	.param .u64 addexchangefourthorder_param_0,
	.param .u64 addexchangefourthorder_param_1,
	.param .u64 addexchangefourthorder_param_2,
	.param .u64 addexchangefourthorder_param_3,
	.param .u64 addexchangefourthorder_param_4,
	.param .u64 addexchangefourthorder_param_5,
	.param .u64 addexchangefourthorder_param_6,
	.param .f32 addexchangefourthorder_param_7,
	.param .u64 addexchangefourthorder_param_8,
	.param .u64 addexchangefourthorder_param_9,
	.param .u64 addexchangefourthorder_param_10,
	.param .f32 addexchangefourthorder_param_11,
	.param .f32 addexchangefourthorder_param_12,
	.param .f32 addexchangefourthorder_param_13,
	.param .u32 addexchangefourthorder_param_14,
	.param .u32 addexchangefourthorder_param_15,
	.param .u32 addexchangefourthorder_param_16,
	.param .u8 addexchangefourthorder_param_17
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<20>;
	.reg .f32 	%f<107>;
	.reg .b32 	%r<71>;
	.reg .b64 	%rd<61>;


	ld.param.u8 	%rs3, [addexchangefourthorder_param_17];
	ld.param.u64 	%rd7, [addexchangefourthorder_param_0];
	ld.param.u64 	%rd8, [addexchangefourthorder_param_1];
	ld.param.u64 	%rd9, [addexchangefourthorder_param_2];
	ld.param.u64 	%rd11, [addexchangefourthorder_param_3];
	ld.param.u64 	%rd12, [addexchangefourthorder_param_4];
	ld.param.u64 	%rd13, [addexchangefourthorder_param_5];
	ld.param.u64 	%rd10, [addexchangefourthorder_param_6];
	ld.param.f32 	%f105, [addexchangefourthorder_param_7];
	ld.param.u64 	%rd14, [addexchangefourthorder_param_8];
	ld.param.u64 	%rd15, [addexchangefourthorder_param_9];
	ld.param.u64 	%rd16, [addexchangefourthorder_param_10];
	ld.param.f32 	%f24, [addexchangefourthorder_param_11];
	ld.param.f32 	%f23, [addexchangefourthorder_param_12];
	ld.param.u32 	%r19, [addexchangefourthorder_param_14];
	ld.param.u32 	%r20, [addexchangefourthorder_param_15];
	ld.param.u32 	%r21, [addexchangefourthorder_param_16];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd15;
	cvta.to.global.u64 	%rd3, %rd16;
	cvta.to.global.u64 	%rd4, %rd13;
	cvta.to.global.u64 	%rd5, %rd12;
	cvta.to.global.u64 	%rd6, %rd11;
	mov.u32 	%r22, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r1, %r23, %r22, %r24;
	mov.u32 	%r25, %ntid.y;
	mov.u32 	%r26, %ctaid.y;
	mov.u32 	%r27, %tid.y;
	mad.lo.s32 	%r2, %r26, %r25, %r27;
	mov.u32 	%r28, %ntid.z;
	mov.u32 	%r29, %ctaid.z;
	mov.u32 	%r30, %tid.z;
	mad.lo.s32 	%r3, %r29, %r28, %r30;
	mul.f32 	%f25, %f24, %f24;
	mov.f32 	%f26, 0f40000000;
	div.rn.f32 	%f1, %f26, %f25;
	mul.f32 	%f27, %f25, %f24;
	mul.f32 	%f28, %f27, %f24;
	div.rn.f32 	%f2, %f26, %f28;
	setp.ge.s32 	%p1, %r1, %r19;
	setp.ge.s32 	%p2, %r2, %r20;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32 	%p4, %r3, %r21;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB0_16;

	mul.lo.s32 	%r4, %r3, %r20;
	add.s32 	%r31, %r4, %r2;
	mul.lo.s32 	%r5, %r31, %r19;
	add.s32 	%r6, %r5, %r1;
	mul.wide.s32 	%rd17, %r6, 4;
	add.s64 	%rd18, %rd6, %rd17;
	add.s64 	%rd19, %rd5, %rd17;
	add.s64 	%rd20, %rd4, %rd17;
	ld.global.nc.f32 	%f3, [%rd18];
	ld.global.nc.f32 	%f4, [%rd19];
	ld.global.nc.f32 	%f5, [%rd20];
	mul.f32 	%f29, %f4, %f4;
	fma.rn.f32 	%f30, %f3, %f3, %f29;
	fma.rn.f32 	%f31, %f5, %f5, %f30;
	setp.eq.f32 	%p6, %f31, 0f00000000;
	@%p6 bra 	$L__BB0_16;

	cvt.s64.s32 	%rd21, %r6;
	add.s64 	%rd22, %rd3, %rd21;
	ld.global.nc.u8 	%rs1, [%rd22];
	and.b16  	%rs2, %rs3, 1;
	setp.eq.s16 	%p7, %rs2, 0;
	add.s32 	%r7, %r1, -2;
	@%p7 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_3;

$L__BB0_4:
	max.s32 	%r68, %r7, 0;
	bra.uni 	$L__BB0_5;

$L__BB0_3:
	rem.s32 	%r32, %r7, %r19;
	add.s32 	%r33, %r32, %r19;
	rem.s32 	%r68, %r33, %r19;

$L__BB0_5:
	cvt.u32.u16 	%r34, %rs1;
	and.b32  	%r35, %r34, 255;
	add.s32 	%r36, %r68, %r5;
	cvt.s64.s32 	%rd23, %r36;
	mul.wide.s32 	%rd24, %r36, 4;
	add.s64 	%rd25, %rd6, %rd24;
	add.s64 	%rd26, %rd5, %rd24;
	add.s64 	%rd27, %rd4, %rd24;
	ld.global.nc.f32 	%f32, [%rd27];
	ld.global.nc.f32 	%f33, [%rd25];
	ld.global.nc.f32 	%f34, [%rd26];
	mul.f32 	%f35, %f34, %f34;
	fma.rn.f32 	%f36, %f33, %f33, %f35;
	fma.rn.f32 	%f37, %f32, %f32, %f36;
	setp.eq.f32 	%p8, %f37, 0f00000000;
	selp.f32 	%f38, %f5, %f32, %p8;
	selp.f32 	%f39, %f4, %f34, %p8;
	selp.f32 	%f40, %f3, %f33, %p8;
	add.s64 	%rd28, %rd3, %rd23;
	ld.global.nc.u8 	%rs5, [%rd28];
	min.u16 	%rs7, %rs5, %rs1;
	cvt.u32.u16 	%r37, %rs7;
	max.u16 	%rs8, %rs5, %rs1;
	cvt.u32.u16 	%r38, %rs8;
	add.s32 	%r39, %r38, 1;
	mul.lo.s32 	%r40, %r39, %r38;
	shr.u32 	%r41, %r40, 1;
	add.s32 	%r42, %r41, %r37;
	mul.wide.s32 	%rd29, %r42, 4;
	add.s64 	%rd30, %rd2, %rd29;
	ld.global.nc.f32 	%f41, [%rd30];
	mul.f32 	%f42, %f2, %f41;
	fma.rn.f32 	%f43, %f42, %f40, 0f00000000;
	fma.rn.f32 	%f44, %f42, %f39, 0f00000000;
	fma.rn.f32 	%f45, %f42, %f38, 0f00000000;
	add.s32 	%r43, %r35, 1;
	mul.lo.s32 	%r44, %r43, %r35;
	shr.u32 	%r45, %r44, 1;
	add.s32 	%r46, %r45, %r35;
	mul.wide.s32 	%rd31, %r46, 4;
	add.s64 	%rd32, %rd2, %rd31;
	mul.f32 	%f46, %f2, 0f40C00000;
	ld.global.nc.f32 	%f47, [%rd32];
	mul.f32 	%f48, %f46, %f47;
	fma.rn.f32 	%f49, %f48, %f3, %f43;
	fma.rn.f32 	%f50, %f48, %f4, %f44;
	fma.rn.f32 	%f51, %f48, %f5, %f45;
	add.s64 	%rd33, %rd1, %rd31;
	add.f32 	%f52, %f1, %f1;
	ld.global.nc.f32 	%f53, [%rd33];
	mul.f32 	%f54, %f52, %f53;
	mul.f32 	%f55, %f3, %f54;
	mul.f32 	%f56, %f4, %f54;
	mul.f32 	%f57, %f5, %f54;
	sub.f32 	%f9, %f49, %f55;
	sub.f32 	%f10, %f50, %f56;
	sub.f32 	%f11, %f51, %f57;
	add.s32 	%r11, %r1, 1;
	@%p7 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_6;

$L__BB0_7:
	add.s32 	%r49, %r19, -1;
	min.s32 	%r69, %r11, %r49;
	bra.uni 	$L__BB0_8;

$L__BB0_6:
	rem.s32 	%r47, %r11, %r19;
	add.s32 	%r48, %r47, %r19;
	rem.s32 	%r69, %r48, %r19;

$L__BB0_8:
	add.s32 	%r50, %r69, %r5;
	cvt.s64.s32 	%rd34, %r50;
	mul.wide.s32 	%rd35, %r50, 4;
	add.s64 	%rd36, %rd6, %rd35;
	add.s64 	%rd37, %rd5, %rd35;
	add.s64 	%rd38, %rd4, %rd35;
	ld.global.nc.f32 	%f58, [%rd38];
	ld.global.nc.f32 	%f59, [%rd36];
	ld.global.nc.f32 	%f60, [%rd37];
	mul.f32 	%f61, %f60, %f60;
	fma.rn.f32 	%f62, %f59, %f59, %f61;
	fma.rn.f32 	%f63, %f58, %f58, %f62;
	setp.eq.f32 	%p10, %f63, 0f00000000;
	selp.f32 	%f64, %f5, %f58, %p10;
	selp.f32 	%f65, %f4, %f60, %p10;
	selp.f32 	%f66, %f3, %f59, %p10;
	add.s64 	%rd39, %rd3, %rd34;
	ld.global.nc.u8 	%rs9, [%rd39];
	min.u16 	%rs12, %rs9, %rs1;
	cvt.u32.u16 	%r51, %rs12;
	max.u16 	%rs13, %rs9, %rs1;
	cvt.u32.u16 	%r52, %rs13;
	add.s32 	%r53, %r52, 1;
	mul.lo.s32 	%r54, %r53, %r52;
	shr.u32 	%r55, %r54, 1;
	add.s32 	%r56, %r55, %r51;
	mul.wide.s32 	%rd40, %r56, 4;
	add.s64 	%rd41, %rd2, %rd40;
	mul.f32 	%f67, %f2, 0f40800000;
	ld.global.nc.f32 	%f68, [%rd41];
	mul.f32 	%f69, %f67, %f68;
	mul.f32 	%f70, %f69, %f66;
	mul.f32 	%f71, %f69, %f65;
	mul.f32 	%f72, %f69, %f64;
	sub.f32 	%f73, %f9, %f70;
	sub.f32 	%f74, %f10, %f71;
	sub.f32 	%f75, %f11, %f72;
	add.s64 	%rd42, %rd1, %rd40;
	ld.global.nc.f32 	%f76, [%rd42];
	mul.f32 	%f77, %f1, %f76;
	fma.rn.f32 	%f12, %f66, %f77, %f73;
	fma.rn.f32 	%f13, %f65, %f77, %f74;
	fma.rn.f32 	%f14, %f64, %f77, %f75;
	and.b16  	%rs14, %rs3, 2;
	setp.eq.s16 	%p11, %rs14, 0;
	add.s32 	%r15, %r2, 2;
	@%p11 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_9;

$L__BB0_10:
	add.s32 	%r59, %r20, -1;
	min.s32 	%r70, %r15, %r59;
	bra.uni 	$L__BB0_11;

$L__BB0_9:
	rem.s32 	%r57, %r15, %r20;
	add.s32 	%r58, %r57, %r20;
	rem.s32 	%r70, %r58, %r20;

$L__BB0_11:
	add.s32 	%r60, %r70, %r4;
	mad.lo.s32 	%r61, %r60, %r19, %r1;
	cvt.s64.s32 	%rd43, %r61;
	mul.wide.s32 	%rd44, %r61, 4;
	add.s64 	%rd45, %rd6, %rd44;
	add.s64 	%rd46, %rd5, %rd44;
	add.s64 	%rd47, %rd4, %rd44;
	ld.global.nc.f32 	%f78, [%rd47];
	ld.global.nc.f32 	%f79, [%rd45];
	ld.global.nc.f32 	%f80, [%rd46];
	mul.f32 	%f81, %f80, %f80;
	fma.rn.f32 	%f82, %f79, %f79, %f81;
	fma.rn.f32 	%f83, %f78, %f78, %f82;
	setp.eq.f32 	%p12, %f83, 0f00000000;
	selp.f32 	%f84, %f5, %f78, %p12;
	selp.f32 	%f85, %f4, %f80, %p12;
	selp.f32 	%f86, %f3, %f79, %p12;
	add.s64 	%rd48, %rd3, %rd43;
	ld.global.nc.u8 	%rs15, [%rd48];
	min.u16 	%rs18, %rs15, %rs1;
	cvt.u32.u16 	%r62, %rs18;
	max.u16 	%rs19, %rs15, %rs1;
	cvt.u32.u16 	%r63, %rs19;
	add.s32 	%r64, %r63, 1;
	mul.lo.s32 	%r65, %r64, %r63;
	shr.u32 	%r66, %r65, 1;
	add.s32 	%r67, %r66, %r62;
	mul.wide.s32 	%rd49, %r67, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.nc.f32 	%f87, [%rd50];
	mul.f32 	%f88, %f23, %f23;
	mul.f32 	%f89, %f88, %f23;
	mul.f32 	%f90, %f89, %f23;
	div.rn.f32 	%f92, %f26, %f90;
	mul.f32 	%f93, %f92, %f87;
	fma.rn.f32 	%f15, %f93, %f86, %f12;
	fma.rn.f32 	%f16, %f93, %f85, %f13;
	fma.rn.f32 	%f17, %f93, %f84, %f14;
	setp.eq.s64 	%p13, %rd10, 0;
	@%p13 bra 	$L__BB0_13;

	cvta.to.global.u64 	%rd51, %rd10;
	add.s64 	%rd53, %rd51, %rd17;
	ld.global.nc.f32 	%f94, [%rd53];
	mul.f32 	%f105, %f94, %f105;

$L__BB0_13:
	setp.eq.f32 	%p14, %f105, 0f00000000;
	mov.f32 	%f106, 0f00000000;
	@%p14 bra 	$L__BB0_15;

	rcp.rn.f32 	%f106, %f105;

$L__BB0_15:
	cvta.to.global.u64 	%rd54, %rd7;
	add.s64 	%rd56, %rd54, %rd17;
	ld.global.f32 	%f96, [%rd56];
	mul.f32 	%f97, %f15, %f106;
	sub.f32 	%f98, %f96, %f97;
	st.global.f32 	[%rd56], %f98;
	cvta.to.global.u64 	%rd57, %rd8;
	add.s64 	%rd58, %rd57, %rd17;
	ld.global.f32 	%f99, [%rd58];
	mul.f32 	%f100, %f16, %f106;
	sub.f32 	%f101, %f99, %f100;
	st.global.f32 	[%rd58], %f101;
	cvta.to.global.u64 	%rd59, %rd9;
	add.s64 	%rd60, %rd59, %rd17;
	ld.global.f32 	%f102, [%rd60];
	mul.f32 	%f103, %f17, %f106;
	sub.f32 	%f104, %f102, %f103;
	st.global.f32 	[%rd60], %f104;

$L__BB0_16:
	ret;

}

`
	addexchangefourthorder_ptx_61 = `
.version 7.4
.target sm_61
.address_size 64

	// .globl	addexchangefourthorder

.visible .entry addexchangefourthorder(
	.param .u64 addexchangefourthorder_param_0,
	.param .u64 addexchangefourthorder_param_1,
	.param .u64 addexchangefourthorder_param_2,
	.param .u64 addexchangefourthorder_param_3,
	.param .u64 addexchangefourthorder_param_4,
	.param .u64 addexchangefourthorder_param_5,
	.param .u64 addexchangefourthorder_param_6,
	.param .f32 addexchangefourthorder_param_7,
	.param .u64 addexchangefourthorder_param_8,
	.param .u64 addexchangefourthorder_param_9,
	.param .u64 addexchangefourthorder_param_10,
	.param .f32 addexchangefourthorder_param_11,
	.param .f32 addexchangefourthorder_param_12,
	.param .f32 addexchangefourthorder_param_13,
	.param .u32 addexchangefourthorder_param_14,
	.param .u32 addexchangefourthorder_param_15,
	.param .u32 addexchangefourthorder_param_16,
	.param .u8 addexchangefourthorder_param_17
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<20>;
	.reg .f32 	%f<107>;
	.reg .b32 	%r<71>;
	.reg .b64 	%rd<61>;


	ld.param.u8 	%rs3, [addexchangefourthorder_param_17];
	ld.param.u64 	%rd7, [addexchangefourthorder_param_0];
	ld.param.u64 	%rd8, [addexchangefourthorder_param_1];
	ld.param.u64 	%rd9, [addexchangefourthorder_param_2];
	ld.param.u64 	%rd11, [addexchangefourthorder_param_3];
	ld.param.u64 	%rd12, [addexchangefourthorder_param_4];
	ld.param.u64 	%rd13, [addexchangefourthorder_param_5];
	ld.param.u64 	%rd10, [addexchangefourthorder_param_6];
	ld.param.f32 	%f105, [addexchangefourthorder_param_7];
	ld.param.u64 	%rd14, [addexchangefourthorder_param_8];
	ld.param.u64 	%rd15, [addexchangefourthorder_param_9];
	ld.param.u64 	%rd16, [addexchangefourthorder_param_10];
	ld.param.f32 	%f24, [addexchangefourthorder_param_11];
	ld.param.f32 	%f23, [addexchangefourthorder_param_12];
	ld.param.u32 	%r19, [addexchangefourthorder_param_14];
	ld.param.u32 	%r20, [addexchangefourthorder_param_15];
	ld.param.u32 	%r21, [addexchangefourthorder_param_16];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd15;
	cvta.to.global.u64 	%rd3, %rd16;
	cvta.to.global.u64 	%rd4, %rd13;
	cvta.to.global.u64 	%rd5, %rd12;
	cvta.to.global.u64 	%rd6, %rd11;
	mov.u32 	%r22, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r1, %r23, %r22, %r24;
	mov.u32 	%r25, %ntid.y;
	mov.u32 	%r26, %ctaid.y;
	mov.u32 	%r27, %tid.y;
	mad.lo.s32 	%r2, %r26, %r25, %r27;
	mov.u32 	%r28, %ntid.z;
	mov.u32 	%r29, %ctaid.z;
	mov.u32 	%r30, %tid.z;
	mad.lo.s32 	%r3, %r29, %r28, %r30;
	mul.f32 	%f25, %f24, %f24;
	mov.f32 	%f26, 0f40000000;
	div.rn.f32 	%f1, %f26, %f25;
	mul.f32 	%f27, %f25, %f24;
	mul.f32 	%f28, %f27, %f24;
	div.rn.f32 	%f2, %f26, %f28;
	setp.ge.s32 	%p1, %r1, %r19;
	setp.ge.s32 	%p2, %r2, %r20;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32 	%p4, %r3, %r21;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB0_16;

	mul.lo.s32 	%r4, %r3, %r20;
	add.s32 	%r31, %r4, %r2;
	mul.lo.s32 	%r5, %r31, %r19;
	add.s32 	%r6, %r5, %r1;
	mul.wide.s32 	%rd17, %r6, 4;
	add.s64 	%rd18, %rd6, %rd17;
	add.s64 	%rd19, %rd5, %rd17;
	add.s64 	%rd20, %rd4, %rd17;
	ld.global.nc.f32 	%f3, [%rd18];
	ld.global.nc.f32 	%f4, [%rd19];
	ld.global.nc.f32 	%f5, [%rd20];
	mul.f32 	%f29, %f4, %f4;
	fma.rn.f32 	%f30, %f3, %f3, %f29;
	fma.rn.f32 	%f31, %f5, %f5, %f30;
	setp.eq.f32 	%p6, %f31, 0f00000000;
	@%p6 bra 	$L__BB0_16;

	cvt.s64.s32 	%rd21, %r6;
	add.s64 	%rd22, %rd3, %rd21;
	ld.global.nc.u8 	%rs1, [%rd22];
	and.b16  	%rs2, %rs3, 1;
	setp.eq.s16 	%p7, %rs2, 0;
	add.s32 	%r7, %r1, -2;
	@%p7 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_3;

$L__BB0_4:
	max.s32 	%r68, %r7, 0;
	bra.uni 	$L__BB0_5;

$L__BB0_3:
	rem.s32 	%r32, %r7, %r19;
	add.s32 	%r33, %r32, %r19;
	rem.s32 	%r68, %r33, %r19;

$L__BB0_5:
	cvt.u32.u16 	%r34, %rs1;
	and.b32  	%r35, %r34, 255;
	add.s32 	%r36, %r68, %r5;
	cvt.s64.s32 	%rd23, %r36;
	mul.wide.s32 	%rd24, %r36, 4;
	add.s64 	%rd25, %rd6, %rd24;
	add.s64 	%rd26, %rd5, %rd24;
	add.s64 	%rd27, %rd4, %rd24;
	ld.global.nc.f32 	%f32, [%rd27];
	ld.global.nc.f32 	%f33, [%rd25];
	ld.global.nc.f32 	%f34, [%rd26];
	mul.f32 	%f35, %f34, %f34;
	fma.rn.f32 	%f36, %f33, %f33, %f35;
	fma.rn.f32 	%f37, %f32, %f32, %f36;
	setp.eq.f32 	%p8, %f37, 0f00000000;
	selp.f32 	%f38, %f5, %f32, %p8;
	selp.f32 	%f39, %f4, %f34, %p8;
	selp.f32 	%f40, %f3, %f33, %p8;
	add.s64 	%rd28, %rd3, %rd23;
	ld.global.nc.u8 	%rs5, [%rd28];
	min.u16 	%rs7, %rs5, %rs1;
	cvt.u32.u16 	%r37, %rs7;
	max.u16 	%rs8, %rs5, %rs1;
	cvt.u32.u16 	%r38, %rs8;
	add.s32 	%r39, %r38, 1;
	mul.lo.s32 	%r40, %r39, %r38;
	shr.u32 	%r41, %r40, 1;
	add.s32 	%r42, %r41, %r37;
	mul.wide.s32 	%rd29, %r42, 4;
	add.s64 	%rd30, %rd2, %rd29;
	ld.global.nc.f32 	%f41, [%rd30];
	mul.f32 	%f42, %f2, %f41;
	fma.rn.f32 	%f43, %f42, %f40, 0f00000000;
	fma.rn.f32 	%f44, %f42, %f39, 0f00000000;
	fma.rn.f32 	%f45, %f42, %f38, 0f00000000;
	add.s32 	%r43, %r35, 1;
	mul.lo.s32 	%r44, %r43, %r35;
	shr.u32 	%r45, %r44, 1;
	add.s32 	%r46, %r45, %r35;
	mul.wide.s32 	%rd31, %r46, 4;
	add.s64 	%rd32, %rd2, %rd31;
	mul.f32 	%f46, %f2, 0f40C00000;
	ld.global.nc.f32 	%f47, [%rd32];
	mul.f32 	%f48, %f46, %f47;
	fma.rn.f32 	%f49, %f48, %f3, %f43;
	fma.rn.f32 	%f50, %f48, %f4, %f44;
	fma.rn.f32 	%f51, %f48, %f5, %f45;
	add.s64 	%rd33, %rd1, %rd31;
	add.f32 	%f52, %f1, %f1;
	ld.global.nc.f32 	%f53, [%rd33];
	mul.f32 	%f54, %f52, %f53;
	mul.f32 	%f55, %f3, %f54;
	mul.f32 	%f56, %f4, %f54;
	mul.f32 	%f57, %f5, %f54;
	sub.f32 	%f9, %f49, %f55;
	sub.f32 	%f10, %f50, %f56;
	sub.f32 	%f11, %f51, %f57;
	add.s32 	%r11, %r1, 1;
	@%p7 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_6;

$L__BB0_7:
	add.s32 	%r49, %r19, -1;
	min.s32 	%r69, %r11, %r49;
	bra.uni 	$L__BB0_8;

$L__BB0_6:
	rem.s32 	%r47, %r11, %r19;
	add.s32 	%r48, %r47, %r19;
	rem.s32 	%r69, %r48, %r19;

$L__BB0_8:
	add.s32 	%r50, %r69, %r5;
	cvt.s64.s32 	%rd34, %r50;
	mul.wide.s32 	%rd35, %r50, 4;
	add.s64 	%rd36, %rd6, %rd35;
	add.s64 	%rd37, %rd5, %rd35;
	add.s64 	%rd38, %rd4, %rd35;
	ld.global.nc.f32 	%f58, [%rd38];
	ld.global.nc.f32 	%f59, [%rd36];
	ld.global.nc.f32 	%f60, [%rd37];
	mul.f32 	%f61, %f60, %f60;
	fma.rn.f32 	%f62, %f59, %f59, %f61;
	fma.rn.f32 	%f63, %f58, %f58, %f62;
	setp.eq.f32 	%p10, %f63, 0f00000000;
	selp.f32 	%f64, %f5, %f58, %p10;
	selp.f32 	%f65, %f4, %f60, %p10;
	selp.f32 	%f66, %f3, %f59, %p10;
	add.s64 	%rd39, %rd3, %rd34;
	ld.global.nc.u8 	%rs9, [%rd39];
	min.u16 	%rs12, %rs9, %rs1;
	cvt.u32.u16 	%r51, %rs12;
	max.u16 	%rs13, %rs9, %rs1;
	cvt.u32.u16 	%r52, %rs13;
	add.s32 	%r53, %r52, 1;
	mul.lo.s32 	%r54, %r53, %r52;
	shr.u32 	%r55, %r54, 1;
	add.s32 	%r56, %r55, %r51;
	mul.wide.s32 	%rd40, %r56, 4;
	add.s64 	%rd41, %rd2, %rd40;
	mul.f32 	%f67, %f2, 0f40800000;
	ld.global.nc.f32 	%f68, [%rd41];
	mul.f32 	%f69, %f67, %f68;
	mul.f32 	%f70, %f69, %f66;
	mul.f32 	%f71, %f69, %f65;
	mul.f32 	%f72, %f69, %f64;
	sub.f32 	%f73, %f9, %f70;
	sub.f32 	%f74, %f10, %f71;
	sub.f32 	%f75, %f11, %f72;
	add.s64 	%rd42, %rd1, %rd40;
	ld.global.nc.f32 	%f76, [%rd42];
	mul.f32 	%f77, %f1, %f76;
	fma.rn.f32 	%f12, %f66, %f77, %f73;
	fma.rn.f32 	%f13, %f65, %f77, %f74;
	fma.rn.f32 	%f14, %f64, %f77, %f75;
	and.b16  	%rs14, %rs3, 2;
	setp.eq.s16 	%p11, %rs14, 0;
	add.s32 	%r15, %r2, 2;
	@%p11 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_9;

$L__BB0_10:
	add.s32 	%r59, %r20, -1;
	min.s32 	%r70, %r15, %r59;
	bra.uni 	$L__BB0_11;

$L__BB0_9:
	rem.s32 	%r57, %r15, %r20;
	add.s32 	%r58, %r57, %r20;
	rem.s32 	%r70, %r58, %r20;

$L__BB0_11:
	add.s32 	%r60, %r70, %r4;
	mad.lo.s32 	%r61, %r60, %r19, %r1;
	cvt.s64.s32 	%rd43, %r61;
	mul.wide.s32 	%rd44, %r61, 4;
	add.s64 	%rd45, %rd6, %rd44;
	add.s64 	%rd46, %rd5, %rd44;
	add.s64 	%rd47, %rd4, %rd44;
	ld.global.nc.f32 	%f78, [%rd47];
	ld.global.nc.f32 	%f79, [%rd45];
	ld.global.nc.f32 	%f80, [%rd46];
	mul.f32 	%f81, %f80, %f80;
	fma.rn.f32 	%f82, %f79, %f79, %f81;
	fma.rn.f32 	%f83, %f78, %f78, %f82;
	setp.eq.f32 	%p12, %f83, 0f00000000;
	selp.f32 	%f84, %f5, %f78, %p12;
	selp.f32 	%f85, %f4, %f80, %p12;
	selp.f32 	%f86, %f3, %f79, %p12;
	add.s64 	%rd48, %rd3, %rd43;
	ld.global.nc.u8 	%rs15, [%rd48];
	min.u16 	%rs18, %rs15, %rs1;
	cvt.u32.u16 	%r62, %rs18;
	max.u16 	%rs19, %rs15, %rs1;
	cvt.u32.u16 	%r63, %rs19;
	add.s32 	%r64, %r63, 1;
	mul.lo.s32 	%r65, %r64, %r63;
	shr.u32 	%r66, %r65, 1;
	add.s32 	%r67, %r66, %r62;
	mul.wide.s32 	%rd49, %r67, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.nc.f32 	%f87, [%rd50];
	mul.f32 	%f88, %f23, %f23;
	mul.f32 	%f89, %f88, %f23;
	mul.f32 	%f90, %f89, %f23;
	div.rn.f32 	%f92, %f26, %f90;
	mul.f32 	%f93, %f92, %f87;
	fma.rn.f32 	%f15, %f93, %f86, %f12;
	fma.rn.f32 	%f16, %f93, %f85, %f13;
	fma.rn.f32 	%f17, %f93, %f84, %f14;
	setp.eq.s64 	%p13, %rd10, 0;
	@%p13 bra 	$L__BB0_13;

	cvta.to.global.u64 	%rd51, %rd10;
	add.s64 	%rd53, %rd51, %rd17;
	ld.global.nc.f32 	%f94, [%rd53];
	mul.f32 	%f105, %f94, %f105;

$L__BB0_13:
	setp.eq.f32 	%p14, %f105, 0f00000000;
	mov.f32 	%f106, 0f00000000;
	@%p14 bra 	$L__BB0_15;

	rcp.rn.f32 	%f106, %f105;

$L__BB0_15:
	cvta.to.global.u64 	%rd54, %rd7;
	add.s64 	%rd56, %rd54, %rd17;
	ld.global.f32 	%f96, [%rd56];
	mul.f32 	%f97, %f15, %f106;
	sub.f32 	%f98, %f96, %f97;
	st.global.f32 	[%rd56], %f98;
	cvta.to.global.u64 	%rd57, %rd8;
	add.s64 	%rd58, %rd57, %rd17;
	ld.global.f32 	%f99, [%rd58];
	mul.f32 	%f100, %f16, %f106;
	sub.f32 	%f101, %f99, %f100;
	st.global.f32 	[%rd58], %f101;
	cvta.to.global.u64 	%rd59, %rd9;
	add.s64 	%rd60, %rd59, %rd17;
	ld.global.f32 	%f102, [%rd60];
	mul.f32 	%f103, %f17, %f106;
	sub.f32 	%f104, %f102, %f103;
	st.global.f32 	[%rd60], %f104;

$L__BB0_16:
	ret;

}

`
	addexchangefourthorder_ptx_62 = `
.version 7.4
.target sm_62
.address_size 64

	// .globl	addexchangefourthorder

.visible .entry addexchangefourthorder(
	.param .u64 addexchangefourthorder_param_0,
	.param .u64 addexchangefourthorder_param_1,
	.param .u64 addexchangefourthorder_param_2,
	.param .u64 addexchangefourthorder_param_3,
	.param .u64 addexchangefourthorder_param_4,
	.param .u64 addexchangefourthorder_param_5,
	.param .u64 addexchangefourthorder_param_6,
	.param .f32 addexchangefourthorder_param_7,
	.param .u64 addexchangefourthorder_param_8,
	.param .u64 addexchangefourthorder_param_9,
	.param .u64 addexchangefourthorder_param_10,
	.param .f32 addexchangefourthorder_param_11,
	.param .f32 addexchangefourthorder_param_12,
	.param .f32 addexchangefourthorder_param_13,
	.param .u32 addexchangefourthorder_param_14,
	.param .u32 addexchangefourthorder_param_15,
	.param .u32 addexchangefourthorder_param_16,
	.param .u8 addexchangefourthorder_param_17
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<20>;
	.reg .f32 	%f<107>;
	.reg .b32 	%r<71>;
	.reg .b64 	%rd<61>;


	ld.param.u8 	%rs3, [addexchangefourthorder_param_17];
	ld.param.u64 	%rd7, [addexchangefourthorder_param_0];
	ld.param.u64 	%rd8, [addexchangefourthorder_param_1];
	ld.param.u64 	%rd9, [addexchangefourthorder_param_2];
	ld.param.u64 	%rd11, [addexchangefourthorder_param_3];
	ld.param.u64 	%rd12, [addexchangefourthorder_param_4];
	ld.param.u64 	%rd13, [addexchangefourthorder_param_5];
	ld.param.u64 	%rd10, [addexchangefourthorder_param_6];
	ld.param.f32 	%f105, [addexchangefourthorder_param_7];
	ld.param.u64 	%rd14, [addexchangefourthorder_param_8];
	ld.param.u64 	%rd15, [addexchangefourthorder_param_9];
	ld.param.u64 	%rd16, [addexchangefourthorder_param_10];
	ld.param.f32 	%f24, [addexchangefourthorder_param_11];
	ld.param.f32 	%f23, [addexchangefourthorder_param_12];
	ld.param.u32 	%r19, [addexchangefourthorder_param_14];
	ld.param.u32 	%r20, [addexchangefourthorder_param_15];
	ld.param.u32 	%r21, [addexchangefourthorder_param_16];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd15;
	cvta.to.global.u64 	%rd3, %rd16;
	cvta.to.global.u64 	%rd4, %rd13;
	cvta.to.global.u64 	%rd5, %rd12;
	cvta.to.global.u64 	%rd6, %rd11;
	mov.u32 	%r22, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r1, %r23, %r22, %r24;
	mov.u32 	%r25, %ntid.y;
	mov.u32 	%r26, %ctaid.y;
	mov.u32 	%r27, %tid.y;
	mad.lo.s32 	%r2, %r26, %r25, %r27;
	mov.u32 	%r28, %ntid.z;
	mov.u32 	%r29, %ctaid.z;
	mov.u32 	%r30, %tid.z;
	mad.lo.s32 	%r3, %r29, %r28, %r30;
	mul.f32 	%f25, %f24, %f24;
	mov.f32 	%f26, 0f40000000;
	div.rn.f32 	%f1, %f26, %f25;
	mul.f32 	%f27, %f25, %f24;
	mul.f32 	%f28, %f27, %f24;
	div.rn.f32 	%f2, %f26, %f28;
	setp.ge.s32 	%p1, %r1, %r19;
	setp.ge.s32 	%p2, %r2, %r20;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32 	%p4, %r3, %r21;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB0_16;

	mul.lo.s32 	%r4, %r3, %r20;
	add.s32 	%r31, %r4, %r2;
	mul.lo.s32 	%r5, %r31, %r19;
	add.s32 	%r6, %r5, %r1;
	mul.wide.s32 	%rd17, %r6, 4;
	add.s64 	%rd18, %rd6, %rd17;
	add.s64 	%rd19, %rd5, %rd17;
	add.s64 	%rd20, %rd4, %rd17;
	ld.global.nc.f32 	%f3, [%rd18];
	ld.global.nc.f32 	%f4, [%rd19];
	ld.global.nc.f32 	%f5, [%rd20];
	mul.f32 	%f29, %f4, %f4;
	fma.rn.f32 	%f30, %f3, %f3, %f29;
	fma.rn.f32 	%f31, %f5, %f5, %f30;
	setp.eq.f32 	%p6, %f31, 0f00000000;
	@%p6 bra 	$L__BB0_16;

	cvt.s64.s32 	%rd21, %r6;
	add.s64 	%rd22, %rd3, %rd21;
	ld.global.nc.u8 	%rs1, [%rd22];
	and.b16  	%rs2, %rs3, 1;
	setp.eq.s16 	%p7, %rs2, 0;
	add.s32 	%r7, %r1, -2;
	@%p7 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_3;

$L__BB0_4:
	max.s32 	%r68, %r7, 0;
	bra.uni 	$L__BB0_5;

$L__BB0_3:
	rem.s32 	%r32, %r7, %r19;
	add.s32 	%r33, %r32, %r19;
	rem.s32 	%r68, %r33, %r19;

$L__BB0_5:
	cvt.u32.u16 	%r34, %rs1;
	and.b32  	%r35, %r34, 255;
	add.s32 	%r36, %r68, %r5;
	cvt.s64.s32 	%rd23, %r36;
	mul.wide.s32 	%rd24, %r36, 4;
	add.s64 	%rd25, %rd6, %rd24;
	add.s64 	%rd26, %rd5, %rd24;
	add.s64 	%rd27, %rd4, %rd24;
	ld.global.nc.f32 	%f32, [%rd27];
	ld.global.nc.f32 	%f33, [%rd25];
	ld.global.nc.f32 	%f34, [%rd26];
	mul.f32 	%f35, %f34, %f34;
	fma.rn.f32 	%f36, %f33, %f33, %f35;
	fma.rn.f32 	%f37, %f32, %f32, %f36;
	setp.eq.f32 	%p8, %f37, 0f00000000;
	selp.f32 	%f38, %f5, %f32, %p8;
	selp.f32 	%f39, %f4, %f34, %p8;
	selp.f32 	%f40, %f3, %f33, %p8;
	add.s64 	%rd28, %rd3, %rd23;
	ld.global.nc.u8 	%rs5, [%rd28];
	min.u16 	%rs7, %rs5, %rs1;
	cvt.u32.u16 	%r37, %rs7;
	max.u16 	%rs8, %rs5, %rs1;
	cvt.u32.u16 	%r38, %rs8;
	add.s32 	%r39, %r38, 1;
	mul.lo.s32 	%r40, %r39, %r38;
	shr.u32 	%r41, %r40, 1;
	add.s32 	%r42, %r41, %r37;
	mul.wide.s32 	%rd29, %r42, 4;
	add.s64 	%rd30, %rd2, %rd29;
	ld.global.nc.f32 	%f41, [%rd30];
	mul.f32 	%f42, %f2, %f41;
	fma.rn.f32 	%f43, %f42, %f40, 0f00000000;
	fma.rn.f32 	%f44, %f42, %f39, 0f00000000;
	fma.rn.f32 	%f45, %f42, %f38, 0f00000000;
	add.s32 	%r43, %r35, 1;
	mul.lo.s32 	%r44, %r43, %r35;
	shr.u32 	%r45, %r44, 1;
	add.s32 	%r46, %r45, %r35;
	mul.wide.s32 	%rd31, %r46, 4;
	add.s64 	%rd32, %rd2, %rd31;
	mul.f32 	%f46, %f2, 0f40C00000;
	ld.global.nc.f32 	%f47, [%rd32];
	mul.f32 	%f48, %f46, %f47;
	fma.rn.f32 	%f49, %f48, %f3, %f43;
	fma.rn.f32 	%f50, %f48, %f4, %f44;
	fma.rn.f32 	%f51, %f48, %f5, %f45;
	add.s64 	%rd33, %rd1, %rd31;
	add.f32 	%f52, %f1, %f1;
	ld.global.nc.f32 	%f53, [%rd33];
	mul.f32 	%f54, %f52, %f53;
	mul.f32 	%f55, %f3, %f54;
	mul.f32 	%f56, %f4, %f54;
	mul.f32 	%f57, %f5, %f54;
	sub.f32 	%f9, %f49, %f55;
	sub.f32 	%f10, %f50, %f56;
	sub.f32 	%f11, %f51, %f57;
	add.s32 	%r11, %r1, 1;
	@%p7 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_6;

$L__BB0_7:
	add.s32 	%r49, %r19, -1;
	min.s32 	%r69, %r11, %r49;
	bra.uni 	$L__BB0_8;

$L__BB0_6:
	rem.s32 	%r47, %r11, %r19;
	add.s32 	%r48, %r47, %r19;
	rem.s32 	%r69, %r48, %r19;

$L__BB0_8:
	add.s32 	%r50, %r69, %r5;
	cvt.s64.s32 	%rd34, %r50;
	mul.wide.s32 	%rd35, %r50, 4;
	add.s64 	%rd36, %rd6, %rd35;
	add.s64 	%rd37, %rd5, %rd35;
	add.s64 	%rd38, %rd4, %rd35;
	ld.global.nc.f32 	%f58, [%rd38];
	ld.global.nc.f32 	%f59, [%rd36];
	ld.global.nc.f32 	%f60, [%rd37];
	mul.f32 	%f61, %f60, %f60;
	fma.rn.f32 	%f62, %f59, %f59, %f61;
	fma.rn.f32 	%f63, %f58, %f58, %f62;
	setp.eq.f32 	%p10, %f63, 0f00000000;
	selp.f32 	%f64, %f5, %f58, %p10;
	selp.f32 	%f65, %f4, %f60, %p10;
	selp.f32 	%f66, %f3, %f59, %p10;
	add.s64 	%rd39, %rd3, %rd34;
	ld.global.nc.u8 	%rs9, [%rd39];
	min.u16 	%rs12, %rs9, %rs1;
	cvt.u32.u16 	%r51, %rs12;
	max.u16 	%rs13, %rs9, %rs1;
	cvt.u32.u16 	%r52, %rs13;
	add.s32 	%r53, %r52, 1;
	mul.lo.s32 	%r54, %r53, %r52;
	shr.u32 	%r55, %r54, 1;
	add.s32 	%r56, %r55, %r51;
	mul.wide.s32 	%rd40, %r56, 4;
	add.s64 	%rd41, %rd2, %rd40;
	mul.f32 	%f67, %f2, 0f40800000;
	ld.global.nc.f32 	%f68, [%rd41];
	mul.f32 	%f69, %f67, %f68;
	mul.f32 	%f70, %f69, %f66;
	mul.f32 	%f71, %f69, %f65;
	mul.f32 	%f72, %f69, %f64;
	sub.f32 	%f73, %f9, %f70;
	sub.f32 	%f74, %f10, %f71;
	sub.f32 	%f75, %f11, %f72;
	add.s64 	%rd42, %rd1, %rd40;
	ld.global.nc.f32 	%f76, [%rd42];
	mul.f32 	%f77, %f1, %f76;
	fma.rn.f32 	%f12, %f66, %f77, %f73;
	fma.rn.f32 	%f13, %f65, %f77, %f74;
	fma.rn.f32 	%f14, %f64, %f77, %f75;
	and.b16  	%rs14, %rs3, 2;
	setp.eq.s16 	%p11, %rs14, 0;
	add.s32 	%r15, %r2, 2;
	@%p11 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_9;

$L__BB0_10:
	add.s32 	%r59, %r20, -1;
	min.s32 	%r70, %r15, %r59;
	bra.uni 	$L__BB0_11;

$L__BB0_9:
	rem.s32 	%r57, %r15, %r20;
	add.s32 	%r58, %r57, %r20;
	rem.s32 	%r70, %r58, %r20;

$L__BB0_11:
	add.s32 	%r60, %r70, %r4;
	mad.lo.s32 	%r61, %r60, %r19, %r1;
	cvt.s64.s32 	%rd43, %r61;
	mul.wide.s32 	%rd44, %r61, 4;
	add.s64 	%rd45, %rd6, %rd44;
	add.s64 	%rd46, %rd5, %rd44;
	add.s64 	%rd47, %rd4, %rd44;
	ld.global.nc.f32 	%f78, [%rd47];
	ld.global.nc.f32 	%f79, [%rd45];
	ld.global.nc.f32 	%f80, [%rd46];
	mul.f32 	%f81, %f80, %f80;
	fma.rn.f32 	%f82, %f79, %f79, %f81;
	fma.rn.f32 	%f83, %f78, %f78, %f82;
	setp.eq.f32 	%p12, %f83, 0f00000000;
	selp.f32 	%f84, %f5, %f78, %p12;
	selp.f32 	%f85, %f4, %f80, %p12;
	selp.f32 	%f86, %f3, %f79, %p12;
	add.s64 	%rd48, %rd3, %rd43;
	ld.global.nc.u8 	%rs15, [%rd48];
	min.u16 	%rs18, %rs15, %rs1;
	cvt.u32.u16 	%r62, %rs18;
	max.u16 	%rs19, %rs15, %rs1;
	cvt.u32.u16 	%r63, %rs19;
	add.s32 	%r64, %r63, 1;
	mul.lo.s32 	%r65, %r64, %r63;
	shr.u32 	%r66, %r65, 1;
	add.s32 	%r67, %r66, %r62;
	mul.wide.s32 	%rd49, %r67, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.nc.f32 	%f87, [%rd50];
	mul.f32 	%f88, %f23, %f23;
	mul.f32 	%f89, %f88, %f23;
	mul.f32 	%f90, %f89, %f23;
	div.rn.f32 	%f92, %f26, %f90;
	mul.f32 	%f93, %f92, %f87;
	fma.rn.f32 	%f15, %f93, %f86, %f12;
	fma.rn.f32 	%f16, %f93, %f85, %f13;
	fma.rn.f32 	%f17, %f93, %f84, %f14;
	setp.eq.s64 	%p13, %rd10, 0;
	@%p13 bra 	$L__BB0_13;

	cvta.to.global.u64 	%rd51, %rd10;
	add.s64 	%rd53, %rd51, %rd17;
	ld.global.nc.f32 	%f94, [%rd53];
	mul.f32 	%f105, %f94, %f105;

$L__BB0_13:
	setp.eq.f32 	%p14, %f105, 0f00000000;
	mov.f32 	%f106, 0f00000000;
	@%p14 bra 	$L__BB0_15;

	rcp.rn.f32 	%f106, %f105;

$L__BB0_15:
	cvta.to.global.u64 	%rd54, %rd7;
	add.s64 	%rd56, %rd54, %rd17;
	ld.global.f32 	%f96, [%rd56];
	mul.f32 	%f97, %f15, %f106;
	sub.f32 	%f98, %f96, %f97;
	st.global.f32 	[%rd56], %f98;
	cvta.to.global.u64 	%rd57, %rd8;
	add.s64 	%rd58, %rd57, %rd17;
	ld.global.f32 	%f99, [%rd58];
	mul.f32 	%f100, %f16, %f106;
	sub.f32 	%f101, %f99, %f100;
	st.global.f32 	[%rd58], %f101;
	cvta.to.global.u64 	%rd59, %rd9;
	add.s64 	%rd60, %rd59, %rd17;
	ld.global.f32 	%f102, [%rd60];
	mul.f32 	%f103, %f17, %f106;
	sub.f32 	%f104, %f102, %f103;
	st.global.f32 	[%rd60], %f104;

$L__BB0_16:
	ret;

}

`
	addexchangefourthorder_ptx_70 = `
.version 7.4
.target sm_70
.address_size 64

	// .globl	addexchangefourthorder

.visible .entry addexchangefourthorder(
	.param .u64 addexchangefourthorder_param_0,
	.param .u64 addexchangefourthorder_param_1,
	.param .u64 addexchangefourthorder_param_2,
	.param .u64 addexchangefourthorder_param_3,
	.param .u64 addexchangefourthorder_param_4,
	.param .u64 addexchangefourthorder_param_5,
	.param .u64 addexchangefourthorder_param_6,
	.param .f32 addexchangefourthorder_param_7,
	.param .u64 addexchangefourthorder_param_8,
	.param .u64 addexchangefourthorder_param_9,
	.param .u64 addexchangefourthorder_param_10,
	.param .f32 addexchangefourthorder_param_11,
	.param .f32 addexchangefourthorder_param_12,
	.param .f32 addexchangefourthorder_param_13,
	.param .u32 addexchangefourthorder_param_14,
	.param .u32 addexchangefourthorder_param_15,
	.param .u32 addexchangefourthorder_param_16,
	.param .u8 addexchangefourthorder_param_17
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<20>;
	.reg .f32 	%f<107>;
	.reg .b32 	%r<71>;
	.reg .b64 	%rd<61>;


	ld.param.u8 	%rs3, [addexchangefourthorder_param_17];
	ld.param.u64 	%rd7, [addexchangefourthorder_param_0];
	ld.param.u64 	%rd8, [addexchangefourthorder_param_1];
	ld.param.u64 	%rd9, [addexchangefourthorder_param_2];
	ld.param.u64 	%rd11, [addexchangefourthorder_param_3];
	ld.param.u64 	%rd12, [addexchangefourthorder_param_4];
	ld.param.u64 	%rd13, [addexchangefourthorder_param_5];
	ld.param.u64 	%rd10, [addexchangefourthorder_param_6];
	ld.param.f32 	%f105, [addexchangefourthorder_param_7];
	ld.param.u64 	%rd14, [addexchangefourthorder_param_8];
	ld.param.u64 	%rd15, [addexchangefourthorder_param_9];
	ld.param.u64 	%rd16, [addexchangefourthorder_param_10];
	ld.param.f32 	%f24, [addexchangefourthorder_param_11];
	ld.param.f32 	%f23, [addexchangefourthorder_param_12];
	ld.param.u32 	%r19, [addexchangefourthorder_param_14];
	ld.param.u32 	%r20, [addexchangefourthorder_param_15];
	ld.param.u32 	%r21, [addexchangefourthorder_param_16];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd15;
	cvta.to.global.u64 	%rd3, %rd16;
	cvta.to.global.u64 	%rd4, %rd13;
	cvta.to.global.u64 	%rd5, %rd12;
	cvta.to.global.u64 	%rd6, %rd11;
	mov.u32 	%r22, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r1, %r23, %r22, %r24;
	mov.u32 	%r25, %ntid.y;
	mov.u32 	%r26, %ctaid.y;
	mov.u32 	%r27, %tid.y;
	mad.lo.s32 	%r2, %r26, %r25, %r27;
	mov.u32 	%r28, %ntid.z;
	mov.u32 	%r29, %ctaid.z;
	mov.u32 	%r30, %tid.z;
	mad.lo.s32 	%r3, %r29, %r28, %r30;
	mul.f32 	%f25, %f24, %f24;
	mov.f32 	%f26, 0f40000000;
	div.rn.f32 	%f1, %f26, %f25;
	mul.f32 	%f27, %f25, %f24;
	mul.f32 	%f28, %f27, %f24;
	div.rn.f32 	%f2, %f26, %f28;
	setp.ge.s32 	%p1, %r1, %r19;
	setp.ge.s32 	%p2, %r2, %r20;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32 	%p4, %r3, %r21;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB0_16;

	mul.lo.s32 	%r4, %r3, %r20;
	add.s32 	%r31, %r4, %r2;
	mul.lo.s32 	%r5, %r31, %r19;
	add.s32 	%r6, %r5, %r1;
	mul.wide.s32 	%rd17, %r6, 4;
	add.s64 	%rd18, %rd6, %rd17;
	add.s64 	%rd19, %rd5, %rd17;
	add.s64 	%rd20, %rd4, %rd17;
	ld.global.nc.f32 	%f3, [%rd18];
	ld.global.nc.f32 	%f4, [%rd19];
	ld.global.nc.f32 	%f5, [%rd20];
	mul.f32 	%f29, %f4, %f4;
	fma.rn.f32 	%f30, %f3, %f3, %f29;
	fma.rn.f32 	%f31, %f5, %f5, %f30;
	setp.eq.f32 	%p6, %f31, 0f00000000;
	@%p6 bra 	$L__BB0_16;

	cvt.s64.s32 	%rd21, %r6;
	add.s64 	%rd22, %rd3, %rd21;
	ld.global.nc.u8 	%rs1, [%rd22];
	and.b16  	%rs2, %rs3, 1;
	setp.eq.s16 	%p7, %rs2, 0;
	add.s32 	%r7, %r1, -2;
	@%p7 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_3;

$L__BB0_4:
	max.s32 	%r68, %r7, 0;
	bra.uni 	$L__BB0_5;

$L__BB0_3:
	rem.s32 	%r32, %r7, %r19;
	add.s32 	%r33, %r32, %r19;
	rem.s32 	%r68, %r33, %r19;

$L__BB0_5:
	cvt.u32.u16 	%r34, %rs1;
	and.b32  	%r35, %r34, 255;
	add.s32 	%r36, %r68, %r5;
	cvt.s64.s32 	%rd23, %r36;
	mul.wide.s32 	%rd24, %r36, 4;
	add.s64 	%rd25, %rd6, %rd24;
	add.s64 	%rd26, %rd5, %rd24;
	add.s64 	%rd27, %rd4, %rd24;
	ld.global.nc.f32 	%f32, [%rd27];
	ld.global.nc.f32 	%f33, [%rd25];
	ld.global.nc.f32 	%f34, [%rd26];
	mul.f32 	%f35, %f34, %f34;
	fma.rn.f32 	%f36, %f33, %f33, %f35;
	fma.rn.f32 	%f37, %f32, %f32, %f36;
	setp.eq.f32 	%p8, %f37, 0f00000000;
	selp.f32 	%f38, %f5, %f32, %p8;
	selp.f32 	%f39, %f4, %f34, %p8;
	selp.f32 	%f40, %f3, %f33, %p8;
	add.s64 	%rd28, %rd3, %rd23;
	ld.global.nc.u8 	%rs5, [%rd28];
	min.u16 	%rs7, %rs5, %rs1;
	cvt.u32.u16 	%r37, %rs7;
	max.u16 	%rs8, %rs5, %rs1;
	cvt.u32.u16 	%r38, %rs8;
	add.s32 	%r39, %r38, 1;
	mul.lo.s32 	%r40, %r39, %r38;
	shr.u32 	%r41, %r40, 1;
	add.s32 	%r42, %r41, %r37;
	mul.wide.s32 	%rd29, %r42, 4;
	add.s64 	%rd30, %rd2, %rd29;
	ld.global.nc.f32 	%f41, [%rd30];
	mul.f32 	%f42, %f2, %f41;
	fma.rn.f32 	%f43, %f42, %f40, 0f00000000;
	fma.rn.f32 	%f44, %f42, %f39, 0f00000000;
	fma.rn.f32 	%f45, %f42, %f38, 0f00000000;
	add.s32 	%r43, %r35, 1;
	mul.lo.s32 	%r44, %r43, %r35;
	shr.u32 	%r45, %r44, 1;
	add.s32 	%r46, %r45, %r35;
	mul.wide.s32 	%rd31, %r46, 4;
	add.s64 	%rd32, %rd2, %rd31;
	mul.f32 	%f46, %f2, 0f40C00000;
	ld.global.nc.f32 	%f47, [%rd32];
	mul.f32 	%f48, %f46, %f47;
	fma.rn.f32 	%f49, %f48, %f3, %f43;
	fma.rn.f32 	%f50, %f48, %f4, %f44;
	fma.rn.f32 	%f51, %f48, %f5, %f45;
	add.s64 	%rd33, %rd1, %rd31;
	add.f32 	%f52, %f1, %f1;
	ld.global.nc.f32 	%f53, [%rd33];
	mul.f32 	%f54, %f52, %f53;
	mul.f32 	%f55, %f3, %f54;
	mul.f32 	%f56, %f4, %f54;
	mul.f32 	%f57, %f5, %f54;
	sub.f32 	%f9, %f49, %f55;
	sub.f32 	%f10, %f50, %f56;
	sub.f32 	%f11, %f51, %f57;
	add.s32 	%r11, %r1, 1;
	@%p7 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_6;

$L__BB0_7:
	add.s32 	%r49, %r19, -1;
	min.s32 	%r69, %r11, %r49;
	bra.uni 	$L__BB0_8;

$L__BB0_6:
	rem.s32 	%r47, %r11, %r19;
	add.s32 	%r48, %r47, %r19;
	rem.s32 	%r69, %r48, %r19;

$L__BB0_8:
	add.s32 	%r50, %r69, %r5;
	cvt.s64.s32 	%rd34, %r50;
	mul.wide.s32 	%rd35, %r50, 4;
	add.s64 	%rd36, %rd6, %rd35;
	add.s64 	%rd37, %rd5, %rd35;
	add.s64 	%rd38, %rd4, %rd35;
	ld.global.nc.f32 	%f58, [%rd38];
	ld.global.nc.f32 	%f59, [%rd36];
	ld.global.nc.f32 	%f60, [%rd37];
	mul.f32 	%f61, %f60, %f60;
	fma.rn.f32 	%f62, %f59, %f59, %f61;
	fma.rn.f32 	%f63, %f58, %f58, %f62;
	setp.eq.f32 	%p10, %f63, 0f00000000;
	selp.f32 	%f64, %f5, %f58, %p10;
	selp.f32 	%f65, %f4, %f60, %p10;
	selp.f32 	%f66, %f3, %f59, %p10;
	add.s64 	%rd39, %rd3, %rd34;
	ld.global.nc.u8 	%rs9, [%rd39];
	min.u16 	%rs12, %rs9, %rs1;
	cvt.u32.u16 	%r51, %rs12;
	max.u16 	%rs13, %rs9, %rs1;
	cvt.u32.u16 	%r52, %rs13;
	add.s32 	%r53, %r52, 1;
	mul.lo.s32 	%r54, %r53, %r52;
	shr.u32 	%r55, %r54, 1;
	add.s32 	%r56, %r55, %r51;
	mul.wide.s32 	%rd40, %r56, 4;
	add.s64 	%rd41, %rd2, %rd40;
	mul.f32 	%f67, %f2, 0f40800000;
	ld.global.nc.f32 	%f68, [%rd41];
	mul.f32 	%f69, %f67, %f68;
	mul.f32 	%f70, %f69, %f66;
	mul.f32 	%f71, %f69, %f65;
	mul.f32 	%f72, %f69, %f64;
	sub.f32 	%f73, %f9, %f70;
	sub.f32 	%f74, %f10, %f71;
	sub.f32 	%f75, %f11, %f72;
	add.s64 	%rd42, %rd1, %rd40;
	ld.global.nc.f32 	%f76, [%rd42];
	mul.f32 	%f77, %f1, %f76;
	fma.rn.f32 	%f12, %f66, %f77, %f73;
	fma.rn.f32 	%f13, %f65, %f77, %f74;
	fma.rn.f32 	%f14, %f64, %f77, %f75;
	and.b16  	%rs14, %rs3, 2;
	setp.eq.s16 	%p11, %rs14, 0;
	add.s32 	%r15, %r2, 2;
	@%p11 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_9;

$L__BB0_10:
	add.s32 	%r59, %r20, -1;
	min.s32 	%r70, %r15, %r59;
	bra.uni 	$L__BB0_11;

$L__BB0_9:
	rem.s32 	%r57, %r15, %r20;
	add.s32 	%r58, %r57, %r20;
	rem.s32 	%r70, %r58, %r20;

$L__BB0_11:
	add.s32 	%r60, %r70, %r4;
	mad.lo.s32 	%r61, %r60, %r19, %r1;
	cvt.s64.s32 	%rd43, %r61;
	mul.wide.s32 	%rd44, %r61, 4;
	add.s64 	%rd45, %rd6, %rd44;
	add.s64 	%rd46, %rd5, %rd44;
	add.s64 	%rd47, %rd4, %rd44;
	ld.global.nc.f32 	%f78, [%rd47];
	ld.global.nc.f32 	%f79, [%rd45];
	ld.global.nc.f32 	%f80, [%rd46];
	mul.f32 	%f81, %f80, %f80;
	fma.rn.f32 	%f82, %f79, %f79, %f81;
	fma.rn.f32 	%f83, %f78, %f78, %f82;
	setp.eq.f32 	%p12, %f83, 0f00000000;
	selp.f32 	%f84, %f5, %f78, %p12;
	selp.f32 	%f85, %f4, %f80, %p12;
	selp.f32 	%f86, %f3, %f79, %p12;
	add.s64 	%rd48, %rd3, %rd43;
	ld.global.nc.u8 	%rs15, [%rd48];
	min.u16 	%rs18, %rs15, %rs1;
	cvt.u32.u16 	%r62, %rs18;
	max.u16 	%rs19, %rs15, %rs1;
	cvt.u32.u16 	%r63, %rs19;
	add.s32 	%r64, %r63, 1;
	mul.lo.s32 	%r65, %r64, %r63;
	shr.u32 	%r66, %r65, 1;
	add.s32 	%r67, %r66, %r62;
	mul.wide.s32 	%rd49, %r67, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.nc.f32 	%f87, [%rd50];
	mul.f32 	%f88, %f23, %f23;
	mul.f32 	%f89, %f88, %f23;
	mul.f32 	%f90, %f89, %f23;
	div.rn.f32 	%f92, %f26, %f90;
	mul.f32 	%f93, %f92, %f87;
	fma.rn.f32 	%f15, %f93, %f86, %f12;
	fma.rn.f32 	%f16, %f93, %f85, %f13;
	fma.rn.f32 	%f17, %f93, %f84, %f14;
	setp.eq.s64 	%p13, %rd10, 0;
	@%p13 bra 	$L__BB0_13;

	cvta.to.global.u64 	%rd51, %rd10;
	add.s64 	%rd53, %rd51, %rd17;
	ld.global.nc.f32 	%f94, [%rd53];
	mul.f32 	%f105, %f94, %f105;

$L__BB0_13:
	setp.eq.f32 	%p14, %f105, 0f00000000;
	mov.f32 	%f106, 0f00000000;
	@%p14 bra 	$L__BB0_15;

	rcp.rn.f32 	%f106, %f105;

$L__BB0_15:
	cvta.to.global.u64 	%rd54, %rd7;
	add.s64 	%rd56, %rd54, %rd17;
	ld.global.f32 	%f96, [%rd56];
	mul.f32 	%f97, %f15, %f106;
	sub.f32 	%f98, %f96, %f97;
	st.global.f32 	[%rd56], %f98;
	cvta.to.global.u64 	%rd57, %rd8;
	add.s64 	%rd58, %rd57, %rd17;
	ld.global.f32 	%f99, [%rd58];
	mul.f32 	%f100, %f16, %f106;
	sub.f32 	%f101, %f99, %f100;
	st.global.f32 	[%rd58], %f101;
	cvta.to.global.u64 	%rd59, %rd9;
	add.s64 	%rd60, %rd59, %rd17;
	ld.global.f32 	%f102, [%rd60];
	mul.f32 	%f103, %f17, %f106;
	sub.f32 	%f104, %f102, %f103;
	st.global.f32 	[%rd60], %f104;

$L__BB0_16:
	ret;

}

`
	addexchangefourthorder_ptx_80 = `
.version 7.4
.target sm_80
.address_size 64

	// .globl	addexchangefourthorder

.visible .entry addexchangefourthorder(
	.param .u64 addexchangefourthorder_param_0,
	.param .u64 addexchangefourthorder_param_1,
	.param .u64 addexchangefourthorder_param_2,
	.param .u64 addexchangefourthorder_param_3,
	.param .u64 addexchangefourthorder_param_4,
	.param .u64 addexchangefourthorder_param_5,
	.param .u64 addexchangefourthorder_param_6,
	.param .f32 addexchangefourthorder_param_7,
	.param .u64 addexchangefourthorder_param_8,
	.param .u64 addexchangefourthorder_param_9,
	.param .u64 addexchangefourthorder_param_10,
	.param .f32 addexchangefourthorder_param_11,
	.param .f32 addexchangefourthorder_param_12,
	.param .f32 addexchangefourthorder_param_13,
	.param .u32 addexchangefourthorder_param_14,
	.param .u32 addexchangefourthorder_param_15,
	.param .u32 addexchangefourthorder_param_16,
	.param .u8 addexchangefourthorder_param_17
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<20>;
	.reg .f32 	%f<107>;
	.reg .b32 	%r<71>;
	.reg .b64 	%rd<61>;


	ld.param.u8 	%rs3, [addexchangefourthorder_param_17];
	ld.param.u64 	%rd7, [addexchangefourthorder_param_0];
	ld.param.u64 	%rd8, [addexchangefourthorder_param_1];
	ld.param.u64 	%rd9, [addexchangefourthorder_param_2];
	ld.param.u64 	%rd11, [addexchangefourthorder_param_3];
	ld.param.u64 	%rd12, [addexchangefourthorder_param_4];
	ld.param.u64 	%rd13, [addexchangefourthorder_param_5];
	ld.param.u64 	%rd10, [addexchangefourthorder_param_6];
	ld.param.f32 	%f105, [addexchangefourthorder_param_7];
	ld.param.u64 	%rd14, [addexchangefourthorder_param_8];
	ld.param.u64 	%rd15, [addexchangefourthorder_param_9];
	ld.param.u64 	%rd16, [addexchangefourthorder_param_10];
	ld.param.f32 	%f24, [addexchangefourthorder_param_11];
	ld.param.f32 	%f23, [addexchangefourthorder_param_12];
	ld.param.u32 	%r19, [addexchangefourthorder_param_14];
	ld.param.u32 	%r20, [addexchangefourthorder_param_15];
	ld.param.u32 	%r21, [addexchangefourthorder_param_16];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd15;
	cvta.to.global.u64 	%rd3, %rd16;
	cvta.to.global.u64 	%rd4, %rd13;
	cvta.to.global.u64 	%rd5, %rd12;
	cvta.to.global.u64 	%rd6, %rd11;
	mov.u32 	%r22, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r1, %r23, %r22, %r24;
	mov.u32 	%r25, %ntid.y;
	mov.u32 	%r26, %ctaid.y;
	mov.u32 	%r27, %tid.y;
	mad.lo.s32 	%r2, %r26, %r25, %r27;
	mov.u32 	%r28, %ntid.z;
	mov.u32 	%r29, %ctaid.z;
	mov.u32 	%r30, %tid.z;
	mad.lo.s32 	%r3, %r29, %r28, %r30;
	mul.f32 	%f25, %f24, %f24;
	mov.f32 	%f26, 0f40000000;
	div.rn.f32 	%f1, %f26, %f25;
	mul.f32 	%f27, %f25, %f24;
	mul.f32 	%f28, %f27, %f24;
	div.rn.f32 	%f2, %f26, %f28;
	setp.ge.s32 	%p1, %r1, %r19;
	setp.ge.s32 	%p2, %r2, %r20;
	or.pred  	%p3, %p1, %p2;
	setp.ge.s32 	%p4, %r3, %r21;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB0_16;

	mul.lo.s32 	%r4, %r3, %r20;
	add.s32 	%r31, %r4, %r2;
	mul.lo.s32 	%r5, %r31, %r19;
	add.s32 	%r6, %r5, %r1;
	mul.wide.s32 	%rd17, %r6, 4;
	add.s64 	%rd18, %rd6, %rd17;
	add.s64 	%rd19, %rd5, %rd17;
	add.s64 	%rd20, %rd4, %rd17;
	ld.global.nc.f32 	%f3, [%rd18];
	ld.global.nc.f32 	%f4, [%rd19];
	ld.global.nc.f32 	%f5, [%rd20];
	mul.f32 	%f29, %f4, %f4;
	fma.rn.f32 	%f30, %f3, %f3, %f29;
	fma.rn.f32 	%f31, %f5, %f5, %f30;
	setp.eq.f32 	%p6, %f31, 0f00000000;
	@%p6 bra 	$L__BB0_16;

	cvt.s64.s32 	%rd21, %r6;
	add.s64 	%rd22, %rd3, %rd21;
	ld.global.nc.u8 	%rs1, [%rd22];
	and.b16  	%rs2, %rs3, 1;
	setp.eq.s16 	%p7, %rs2, 0;
	add.s32 	%r7, %r1, -2;
	@%p7 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_3;

$L__BB0_4:
	max.s32 	%r68, %r7, 0;
	bra.uni 	$L__BB0_5;

$L__BB0_3:
	rem.s32 	%r32, %r7, %r19;
	add.s32 	%r33, %r32, %r19;
	rem.s32 	%r68, %r33, %r19;

$L__BB0_5:
	cvt.u32.u16 	%r34, %rs1;
	and.b32  	%r35, %r34, 255;
	add.s32 	%r36, %r68, %r5;
	cvt.s64.s32 	%rd23, %r36;
	mul.wide.s32 	%rd24, %r36, 4;
	add.s64 	%rd25, %rd6, %rd24;
	add.s64 	%rd26, %rd5, %rd24;
	add.s64 	%rd27, %rd4, %rd24;
	ld.global.nc.f32 	%f32, [%rd27];
	ld.global.nc.f32 	%f33, [%rd25];
	ld.global.nc.f32 	%f34, [%rd26];
	mul.f32 	%f35, %f34, %f34;
	fma.rn.f32 	%f36, %f33, %f33, %f35;
	fma.rn.f32 	%f37, %f32, %f32, %f36;
	setp.eq.f32 	%p8, %f37, 0f00000000;
	selp.f32 	%f38, %f5, %f32, %p8;
	selp.f32 	%f39, %f4, %f34, %p8;
	selp.f32 	%f40, %f3, %f33, %p8;
	add.s64 	%rd28, %rd3, %rd23;
	ld.global.nc.u8 	%rs5, [%rd28];
	min.u16 	%rs7, %rs5, %rs1;
	cvt.u32.u16 	%r37, %rs7;
	max.u16 	%rs8, %rs5, %rs1;
	cvt.u32.u16 	%r38, %rs8;
	add.s32 	%r39, %r38, 1;
	mul.lo.s32 	%r40, %r39, %r38;
	shr.u32 	%r41, %r40, 1;
	add.s32 	%r42, %r41, %r37;
	mul.wide.s32 	%rd29, %r42, 4;
	add.s64 	%rd30, %rd2, %rd29;
	ld.global.nc.f32 	%f41, [%rd30];
	mul.f32 	%f42, %f2, %f41;
	fma.rn.f32 	%f43, %f42, %f40, 0f00000000;
	fma.rn.f32 	%f44, %f42, %f39, 0f00000000;
	fma.rn.f32 	%f45, %f42, %f38, 0f00000000;
	add.s32 	%r43, %r35, 1;
	mul.lo.s32 	%r44, %r43, %r35;
	shr.u32 	%r45, %r44, 1;
	add.s32 	%r46, %r45, %r35;
	mul.wide.s32 	%rd31, %r46, 4;
	add.s64 	%rd32, %rd2, %rd31;
	mul.f32 	%f46, %f2, 0f40C00000;
	ld.global.nc.f32 	%f47, [%rd32];
	mul.f32 	%f48, %f46, %f47;
	fma.rn.f32 	%f49, %f48, %f3, %f43;
	fma.rn.f32 	%f50, %f48, %f4, %f44;
	fma.rn.f32 	%f51, %f48, %f5, %f45;
	add.s64 	%rd33, %rd1, %rd31;
	add.f32 	%f52, %f1, %f1;
	ld.global.nc.f32 	%f53, [%rd33];
	mul.f32 	%f54, %f52, %f53;
	mul.f32 	%f55, %f3, %f54;
	mul.f32 	%f56, %f4, %f54;
	mul.f32 	%f57, %f5, %f54;
	sub.f32 	%f9, %f49, %f55;
	sub.f32 	%f10, %f50, %f56;
	sub.f32 	%f11, %f51, %f57;
	add.s32 	%r11, %r1, 1;
	@%p7 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_6;

$L__BB0_7:
	add.s32 	%r49, %r19, -1;
	min.s32 	%r69, %r11, %r49;
	bra.uni 	$L__BB0_8;

$L__BB0_6:
	rem.s32 	%r47, %r11, %r19;
	add.s32 	%r48, %r47, %r19;
	rem.s32 	%r69, %r48, %r19;

$L__BB0_8:
	add.s32 	%r50, %r69, %r5;
	cvt.s64.s32 	%rd34, %r50;
	mul.wide.s32 	%rd35, %r50, 4;
	add.s64 	%rd36, %rd6, %rd35;
	add.s64 	%rd37, %rd5, %rd35;
	add.s64 	%rd38, %rd4, %rd35;
	ld.global.nc.f32 	%f58, [%rd38];
	ld.global.nc.f32 	%f59, [%rd36];
	ld.global.nc.f32 	%f60, [%rd37];
	mul.f32 	%f61, %f60, %f60;
	fma.rn.f32 	%f62, %f59, %f59, %f61;
	fma.rn.f32 	%f63, %f58, %f58, %f62;
	setp.eq.f32 	%p10, %f63, 0f00000000;
	selp.f32 	%f64, %f5, %f58, %p10;
	selp.f32 	%f65, %f4, %f60, %p10;
	selp.f32 	%f66, %f3, %f59, %p10;
	add.s64 	%rd39, %rd3, %rd34;
	ld.global.nc.u8 	%rs9, [%rd39];
	min.u16 	%rs12, %rs9, %rs1;
	cvt.u32.u16 	%r51, %rs12;
	max.u16 	%rs13, %rs9, %rs1;
	cvt.u32.u16 	%r52, %rs13;
	add.s32 	%r53, %r52, 1;
	mul.lo.s32 	%r54, %r53, %r52;
	shr.u32 	%r55, %r54, 1;
	add.s32 	%r56, %r55, %r51;
	mul.wide.s32 	%rd40, %r56, 4;
	add.s64 	%rd41, %rd2, %rd40;
	mul.f32 	%f67, %f2, 0f40800000;
	ld.global.nc.f32 	%f68, [%rd41];
	mul.f32 	%f69, %f67, %f68;
	mul.f32 	%f70, %f69, %f66;
	mul.f32 	%f71, %f69, %f65;
	mul.f32 	%f72, %f69, %f64;
	sub.f32 	%f73, %f9, %f70;
	sub.f32 	%f74, %f10, %f71;
	sub.f32 	%f75, %f11, %f72;
	add.s64 	%rd42, %rd1, %rd40;
	ld.global.nc.f32 	%f76, [%rd42];
	mul.f32 	%f77, %f1, %f76;
	fma.rn.f32 	%f12, %f66, %f77, %f73;
	fma.rn.f32 	%f13, %f65, %f77, %f74;
	fma.rn.f32 	%f14, %f64, %f77, %f75;
	and.b16  	%rs14, %rs3, 2;
	setp.eq.s16 	%p11, %rs14, 0;
	add.s32 	%r15, %r2, 2;
	@%p11 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_9;

$L__BB0_10:
	add.s32 	%r59, %r20, -1;
	min.s32 	%r70, %r15, %r59;
	bra.uni 	$L__BB0_11;

$L__BB0_9:
	rem.s32 	%r57, %r15, %r20;
	add.s32 	%r58, %r57, %r20;
	rem.s32 	%r70, %r58, %r20;

$L__BB0_11:
	add.s32 	%r60, %r70, %r4;
	mad.lo.s32 	%r61, %r60, %r19, %r1;
	cvt.s64.s32 	%rd43, %r61;
	mul.wide.s32 	%rd44, %r61, 4;
	add.s64 	%rd45, %rd6, %rd44;
	add.s64 	%rd46, %rd5, %rd44;
	add.s64 	%rd47, %rd4, %rd44;
	ld.global.nc.f32 	%f78, [%rd47];
	ld.global.nc.f32 	%f79, [%rd45];
	ld.global.nc.f32 	%f80, [%rd46];
	mul.f32 	%f81, %f80, %f80;
	fma.rn.f32 	%f82, %f79, %f79, %f81;
	fma.rn.f32 	%f83, %f78, %f78, %f82;
	setp.eq.f32 	%p12, %f83, 0f00000000;
	selp.f32 	%f84, %f5, %f78, %p12;
	selp.f32 	%f85, %f4, %f80, %p12;
	selp.f32 	%f86, %f3, %f79, %p12;
	add.s64 	%rd48, %rd3, %rd43;
	ld.global.nc.u8 	%rs15, [%rd48];
	min.u16 	%rs18, %rs15, %rs1;
	cvt.u32.u16 	%r62, %rs18;
	max.u16 	%rs19, %rs15, %rs1;
	cvt.u32.u16 	%r63, %rs19;
	add.s32 	%r64, %r63, 1;
	mul.lo.s32 	%r65, %r64, %r63;
	shr.u32 	%r66, %r65, 1;
	add.s32 	%r67, %r66, %r62;
	mul.wide.s32 	%rd49, %r67, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.nc.f32 	%f87, [%rd50];
	mul.f32 	%f88, %f23, %f23;
	mul.f32 	%f89, %f88, %f23;
	mul.f32 	%f90, %f89, %f23;
	div.rn.f32 	%f92, %f26, %f90;
	mul.f32 	%f93, %f92, %f87;
	fma.rn.f32 	%f15, %f93, %f86, %f12;
	fma.rn.f32 	%f16, %f93, %f85, %f13;
	fma.rn.f32 	%f17, %f93, %f84, %f14;
	setp.eq.s64 	%p13, %rd10, 0;
	@%p13 bra 	$L__BB0_13;

	cvta.to.global.u64 	%rd51, %rd10;
	add.s64 	%rd53, %rd51, %rd17;
	ld.global.nc.f32 	%f94, [%rd53];
	mul.f32 	%f105, %f94, %f105;

$L__BB0_13:
	setp.eq.f32 	%p14, %f105, 0f00000000;
	mov.f32 	%f106, 0f00000000;
	@%p14 bra 	$L__BB0_15;

	rcp.rn.f32 	%f106, %f105;

$L__BB0_15:
	cvta.to.global.u64 	%rd54, %rd7;
	add.s64 	%rd56, %rd54, %rd17;
	ld.global.f32 	%f96, [%rd56];
	mul.f32 	%f97, %f15, %f106;
	sub.f32 	%f98, %f96, %f97;
	st.global.f32 	[%rd56], %f98;
	cvta.to.global.u64 	%rd57, %rd8;
	add.s64 	%rd58, %rd57, %rd17;
	ld.global.f32 	%f99, [%rd58];
	mul.f32 	%f100, %f16, %f106;
	sub.f32 	%f101, %f99, %f100;
	st.global.f32 	[%rd58], %f101;
	cvta.to.global.u64 	%rd59, %rd9;
	add.s64 	%rd60, %rd59, %rd17;
	ld.global.f32 	%f102, [%rd60];
	mul.f32 	%f103, %f17, %f106;
	sub.f32 	%f104, %f102, %f103;
	st.global.f32 	[%rd60], %f104;

$L__BB0_16:
	ret;

}

`
)
